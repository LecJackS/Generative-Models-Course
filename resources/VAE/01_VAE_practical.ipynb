{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Variational Autoencoder (VAE) - Practical Session"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd10f045",
      "metadata": {},
      "source": [
        "## Section 0 - Recap: What is a VAE?\n",
        "\n",
        "### 0.1 The problem: generating high-dimensional data\n",
        "\n",
        "We want a model that can **generate new high-dimensional data points** that look like the ones in our dataset.  \n",
        "Formally, we would like to model a complex distribution over data $p(x)$, where $x$ is a point in a high-dimensional space $x \\in \\mathbb{R}^D$.\n",
        "\n",
        "In general, $x$ could be an image, an audio clip, a text token sequence, a molecule, or some other structured object.\n",
        "\n",
        "Directly modeling $p(x)$ is hard. Instead, we introduce a lower-dimensional **latent variable** $z$ which is supposed to capture abstract factors of variation (for example: style, stroke thickness, digit identity).\n",
        "\n",
        "In a VAE, we define a generative model\n",
        "$p(x, z) = p(z)\\, p(x \\mid z)$,\n",
        "with a simple prior $p(z)$ (usually a standard normal) and a neural network decoder for $p(x \\mid z)$.\n",
        "Later, we will approximate the intractable posterior $p(z \\mid x)$ with a neural network encoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2adf9b1",
      "metadata": {},
      "source": [
        "### 0.2 Generative story of a VAE: prior and decoder\n",
        "\n",
        "At the level of the probabilistic model, a VAE is just a latent-variable model.\n",
        "\n",
        "We assume a simple prior over latent variables:\n",
        "- Prior (latent space): $p(z) = \\mathcal{N}(0, I)$\n",
        "\n",
        "Given $z$, we decode back to data space with a neural network that defines a conditional distribution:\n",
        "- Decoder or generative model: $p_\\theta(x \\mid z)$\n",
        "\n",
        "For image data, common choices for $p_\\theta(x \\mid z)$ are:\n",
        "- A factorized Bernoulli over pixels, which leads to a binary cross-entropy reconstruction term (often used for binarized MNIST).\n",
        "- A Gaussian with fixed variance, which leads to a mean squared error reconstruction term.\n",
        "\n",
        "Together, prior and decoder define the joint distribution\n",
        "$$\n",
        "p_\\theta(x, z) = p(z)\\, p_\\theta(x \\mid z).\n",
        "$$\n",
        "\n",
        "The generative procedure is:\n",
        "1. Sample $z \\sim p(z)$ (for example, a $2$-dimensional or $10$-dimensional standard Gaussian).\n",
        "2. Compute the decoder output $p_\\theta(x \\mid z)$ using a neural network.\n",
        "3. Either sample $x$ from that distribution or take its mean as a generated data point.\n",
        "\n",
        "Intuitively, the prior shapes the global structure of the latent space, and the decoder learns how to turn latent codes into realistic data. If learning goes well, nearby points in latent space $z$ should decode to similar-looking samples.\n",
        "\n",
        "---\n",
        "\n",
        "### 0.3 Inference model: encoder and approximate posterior\n",
        "\n",
        "For learning, we would like to use the posterior $p_\\theta(z \\mid x)$, but it is intractable for nonlinear neural decoders.\n",
        "\n",
        "The VAE introduces an encoder network that defines a tractable **approximate posterior**:\n",
        "- Encoder or inference model: $q_\\phi(z \\mid x)$\n",
        "\n",
        "We parameterize $q_\\phi(z \\mid x)$ as a diagonal Gaussian whose parameters depend on $x$:\n",
        "- Mean vector $\\mu_\\phi(x)$\n",
        "- Log-variance vector $\\log \\sigma_\\phi^2(x)$\n",
        "\n",
        "so that\n",
        "$$\n",
        "q_\\phi(z \\mid x) = \\mathcal{N}\\big(z;\\, \\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x))\\big).\n",
        "$$\n",
        "\n",
        "Reasons for this parameterization:\n",
        "- We use a diagonal covariance for simplicity and computational efficiency.\n",
        "- We output $\\log \\sigma_\\phi^2(x)$ rather than $\\sigma_\\phi(x)$ directly so that the network output is unconstrained, and we can recover positive variances via an exponential.\n",
        "\n",
        "Later in the notebook, we will **sample** from $q_\\phi(z \\mid x)$ using the reparameterization trick:\n",
        "$$\n",
        "\\epsilon \\sim \\mathcal{N}(0, I), \\quad\n",
        "z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon,\n",
        "$$\n",
        "which allows gradients to flow through $z$ during backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "### 0.4 Training objective: ELBO and its interpretation\n",
        "\n",
        "We would like to learn the parameters $(\\theta, \\phi)$ by maximizing the marginal log-likelihood $\\log p_\\theta(x)$ over the dataset. Directly optimizing $\\log p_\\theta(x)$ is intractable, so VAEs optimize a tractable lower bound instead.\n",
        "\n",
        "For each data point $x$, the **Evidence Lower Bound (ELBO)** is\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\phi; x)\n",
        "=\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\,\\Vert\\, p(z)\\big),\n",
        "$$\n",
        "and it satisfies\n",
        "$$\n",
        "\\log p_\\theta(x)\n",
        "=\n",
        "\\mathcal{L}(\\theta, \\phi; x)\n",
        "+\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\,\\Vert\\, p_\\theta(z \\mid x)\\big)\n",
        "\\ge \\mathcal{L}(\\theta, \\phi; x),\n",
        "$$\n",
        "because the KL divergence is always non-negative.\n",
        "\n",
        "The ELBO decomposes into two terms:\n",
        "\n",
        "- **Reconstruction term**\n",
        "  $$\n",
        "  \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "  $$\n",
        "  This encourages the decoder to reconstruct $x$ accurately from samples of $z$.  \n",
        "  In practice:\n",
        "  - If $p_\\theta(x \\mid z)$ is Bernoulli per pixel, this becomes a sum of binary cross-entropies.\n",
        "  - If $p_\\theta(x \\mid z)$ is Gaussian with fixed variance, this becomes a scaled mean squared error.\n",
        "\n",
        "- **KL term**\n",
        "  $$\n",
        "  \\mathrm{KL}\\big(q_\\phi(z \\mid x) \\,\\Vert\\, p(z)\\big)\n",
        "  $$\n",
        "  This encourages the approximate posterior to stay close to the prior. It regularizes the encoder so that:\n",
        "  - The latent codes do not drift too far from the prior.\n",
        "  - The latent space stays smooth and well-organized, which is crucial for sampling and interpolation.\n",
        "\n",
        "You can think of a VAE as a **regularized autoencoder**:\n",
        "- Like a classic autoencoder, it wants good reconstructions.\n",
        "- Unlike a classic autoencoder, it also wants a well-behaved latent distribution that roughly matches $p(z)$.\n",
        "\n",
        "---\n",
        "\n",
        "### 0.5 From ELBO to the loss we implement\n",
        "\n",
        "In code, we usually **minimize** a loss rather than maximize the ELBO. For a single data point $x$ we define\n",
        "$$\n",
        "\\mathcal{L}_{\\text{VAE}}(x)\n",
        "=\n",
        "- \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "+\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\,\\Vert\\, p(z)\\big),\n",
        "$$\n",
        "which is just the negative ELBO:\n",
        "- The first term becomes a reconstruction loss.\n",
        "- The second term is the KL penalty.\n",
        "\n",
        "Sometimes a weighting factor $\\beta$ is inserted in front of the KL term (a $\\beta$-VAE). We will start with $\\beta = 1$ (the standard VAE objective) and optionally experiment with other values later.\n",
        "\n",
        "In practice:\n",
        "- The expectation over $q_\\phi(z \\mid x)$ is approximated with one or a few Monte Carlo samples using the reparameterization trick.\n",
        "- The KL term between two Gaussians (diagonal covariance vs standard normal) has a closed-form expression, which we will implement directly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad5be9e8",
      "metadata": {},
      "source": [
        "### 0.6 Toy example: how the KL term shapes a 2D latent space\n",
        "\n",
        "In the VAE, the encoder outputs a Gaussian approximate posterior $q_\\phi(z \\mid x)$\n",
        "in the latent space, and we penalize its distance to the prior $p(z)$ with a KL term.\n",
        "\n",
        "To build intuition, we now look at a 2D latent space.\n",
        "\n",
        "We define:\n",
        "- Prior: $p(z) = \\mathcal{N}(z; 0, I_2)$, a standard 2D Gaussian.\n",
        "- Approximate posterior: $q(z) = \\mathcal{N}(z; \\mu, \\Sigma)$\n",
        "  with diagonal covariance $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2)$.\n",
        "\n",
        "This is exactly what a VAE with a 2D latent and diagonal covariance does per data point.\n",
        "\n",
        "For this simple case, the KL divergence has the closed form\n",
        "$$\n",
        "\\mathrm{KL}(q \\Vert p)\n",
        "=\n",
        "\\frac{1}{2}\n",
        "\\sum_{i=1}^2\n",
        "\\big(\n",
        "\\sigma_i^2 + \\mu_i^2 - 1 - \\log \\sigma_i^2\n",
        "\\big).\n",
        "$$\n",
        "\n",
        "Use the sliders below to change the components of\n",
        "$\\mu = (\\mu_1, \\mu_2)$ and the log-variances\n",
        "$\\log \\sigma_1^2, \\log \\sigma_2^2$.\n",
        "\n",
        "You will see:\n",
        "- The prior $p(z)$ as circular level curves.\n",
        "- The approximate posterior $q(z)$ as elliptical level curves.\n",
        "- How the KL penalty reacts when you move or stretch $q(z)$\n",
        "  relative to $p(z)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9de2a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable if in google colab\n",
        "\n",
        "# !pip install torch torchvision matplotlib seaborn numpy scipy tqdm pillow\n",
        "# !pip -q install \"ipywidgets>=7,<8\"\n",
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03533f2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, FloatSlider\n",
        "\n",
        "# KL(q || p) for 2D diagonal Gaussian vs. standard normal\n",
        "def kl_standard_normal_2d(mu, logvar):\n",
        "    sigma2 = torch.exp(logvar)\n",
        "    # TODO: implement the closed-form KL **PER DIMENSION**\n",
        "    kl_per_dim = None # TODO\n",
        "    return kl_per_dim.sum(), kl_per_dim\n",
        "\n",
        "# Log-density of 2D diagonal Gaussian\n",
        "def log_diag_gaussian_pdf_2d(z1, z2, mu, sigma2):\n",
        "    mu1, mu2 = mu[0], mu[1]\n",
        "    s1, s2 = sigma2[0], sigma2[1]\n",
        "    quad = (z1 - mu1)**2 / s1 + (z2 - mu2)**2 / s2\n",
        "    log_Z = torch.log((2 * torch.pi)**2 * s1 * s2)\n",
        "    return -0.5 * quad - 0.5 * log_Z\n",
        "\n",
        "# Grid\n",
        "z_min, z_max, num_points = -4.0, 4.0, 201\n",
        "z1_vals = torch.linspace(z_min, z_max, num_points, dtype=torch.float64)\n",
        "z2_vals = torch.linspace(z_min, z_max, num_points, dtype=torch.float64)\n",
        "z1, z2 = torch.meshgrid(z1_vals, z2_vals, indexing=\"xy\")\n",
        "\n",
        "def plot_2d_kl_and_contours(mu1=0.0, mu2=0.0, logvar1=0.0, logvar2=0.0):\n",
        "    mu = torch.tensor([mu1, mu2], dtype=torch.float64)\n",
        "    logvar = torch.tensor([logvar1, logvar2], dtype=torch.float64)\n",
        "    sigma2 = torch.exp(logvar)\n",
        "\n",
        "    kl_total, kl_per_dim = kl_standard_normal_2d(mu, logvar)\n",
        "\n",
        "    mu_prior = torch.tensor([0.0, 0.0], dtype=torch.float64)\n",
        "    sigma2_prior = torch.tensor([1.0, 1.0], dtype=torch.float64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_p = log_diag_gaussian_pdf_2d(z1, z2, mu_prior, sigma2_prior)\n",
        "        log_q = log_diag_gaussian_pdf_2d(z1, z2, mu, sigma2)\n",
        "        p_vals = torch.exp(log_p)\n",
        "        q_vals = torch.exp(log_q)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    cs_p = plt.contour(\n",
        "        z1.numpy(), z2.numpy(), p_vals.numpy(),\n",
        "        levels=5, linestyles=\"dashed\", linewidths=1.0, alpha=0.8,\n",
        "    )\n",
        "    cs_q = plt.contour(\n",
        "        z1.numpy(), z2.numpy(), q_vals.numpy(),\n",
        "        levels=5, linewidths=1.5, alpha=0.8,\n",
        "    )\n",
        "\n",
        "    # Manual legend handles (no .collections access)\n",
        "    prior_handle = plt.Line2D([], [], color='k', linestyle='--', label='p(z) = N(0,I)')\n",
        "    post_handle = plt.Line2D([], [], color='C0', label='q(z) = N(mu, diag)')\n",
        "    mean_prior_handle = plt.Line2D([], [], marker='x', color='black', linestyle='', label='prior mean')\n",
        "    mean_q_handle = plt.Line2D([], [], marker='o', color='red', linestyle='', label='q mean')\n",
        "\n",
        "    plt.scatter([0.0], [0.0], color=\"black\", s=30, marker=\"x\")\n",
        "    plt.scatter([mu1], [mu2], color=\"red\", s=30, marker=\"o\")\n",
        "\n",
        "    plt.title(\n",
        "        f\"KL(q || p) = {kl_total.item():.3f}  \"\n",
        "        f\"(per dim: {kl_per_dim[0].item():.3f}, {kl_per_dim[1].item():.3f})\"\n",
        "    )\n",
        "    plt.xlabel(\"z1\")\n",
        "    plt.ylabel(\"z2\")\n",
        "    plt.xlim(z_min, z_max)\n",
        "    plt.ylim(z_min, z_max)\n",
        "    plt.gca().set_aspect(\"equal\", \"box\")\n",
        "    plt.legend(handles=[prior_handle, post_handle, mean_prior_handle, mean_q_handle], loc=\"upper right\")\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive sliders\n",
        "interact(\n",
        "    plot_2d_kl_and_contours,\n",
        "    mu1=FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description=\"mu1\"),\n",
        "    mu2=FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description=\"mu2\"),\n",
        "    logvar1=FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description=\"log sigma1^2\"),\n",
        "    logvar2=FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description=\"log sigma2^2\"),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "667e9534",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "1. Fix `log sigma1^2 = 0`, `log sigma2^2 = 0` (so $\\sigma_1^2 = \\sigma_2^2 = 1$) and move `mu1`, `mu2`.\n",
        "   - How does the KL change as a function of the distance $\\|\\mu\\|_2$ between the two means?\n",
        "   - What do you observe about the shape of $q(z)$ compared to $p(z)$?\n",
        "\n",
        "2. Fix `mu1 = 0`, `mu2 = 0` and change `log sigma1^2`, `log sigma2^2`.\n",
        "   - What happens when one variance is very small and the other is close to 1?\n",
        "   - What happens when one variance is very large and the other is close to 1?\n",
        "   - For which values of $(\\mu, \\sigma_1^2, \\sigma_2^2)$ is the KL close to 0?\n",
        "\n",
        "Relate your observations to the VAE objective:\n",
        "- The KL term encourages $\\mu_\\phi(x)$ to stay near 0 and $\\sigma_\\phi^2(x)$ to stay near 1 in every dimension.\n",
        "- The reconstruction term will push these parameters away from the prior only when this improves reconstruction.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb1a2d9",
      "metadata": {},
      "source": [
        "### Quick check before continuing\n",
        "\n",
        "Use these to confirm your mental model. Try to answer out loud or in writing before looking at the answers.\n",
        "\n",
        "1. Why do we introduce a latent variable $z$ instead of modeling $p(x)$ directly?\n",
        "\n",
        "2. What are the complementary roles of the encoder $q_\\phi(z \\mid x)$ and the decoder $p_\\theta(x \\mid z)$?\n",
        "\n",
        "3. In the ELBO\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\phi; x)\n",
        "=\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big),\n",
        "$$\n",
        "what do the two terms encourage?\n",
        "\n",
        "4. If we drop the KL term entirely and only optimize the reconstruction term, what behavior should you expect during training and sampling?\n",
        "\n",
        "5. In a $\\beta$-VAE, the objective is\n",
        "$$\n",
        "\\mathcal{L}_\\beta(\\theta, \\phi; x)\n",
        "=\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\beta \\, \\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big).\n",
        "$$\n",
        "What happens qualitatively if $\\beta$ is chosen too large?\n",
        "\n",
        "6. In the 2D toy example, we use a prior\n",
        "$$\n",
        "p(z) = \\mathcal{N}(z; 0, I_2)\n",
        "$$\n",
        "and an approximate posterior\n",
        "$$\n",
        "q(z) = \\mathcal{N}(z; \\mu, \\Sigma),\n",
        "\\quad\n",
        "\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2).\n",
        "$$\n",
        "For which values of $\\mu$ and $(\\sigma_1^2, \\sigma_2^2)$ is $\\mathrm{KL}(q \\Vert p)$ equal to $0$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17879e48",
      "metadata": {},
      "source": [
        "### Answers\n",
        "\n",
        "1. **Why do we introduce a latent variable $z$ instead of modeling $p(x)$ directly?**\n",
        "\n",
        "Directly modeling $p(x)$ in a high-dimensional space is hard: we need a flexible density and must normalize it over $\\mathbb{R}^D$.  \n",
        "By introducing $z$ and factorizing $p_\\theta(x, z) = p(z)\\, p_\\theta(x \\mid z)$, we:\n",
        "- put the complexity into the conditional $p_\\theta(x \\mid z)$ (implemented by a neural network), and  \n",
        "- keep $p(z)$ simple (for example, $z \\sim \\mathcal{N}(0, I)$),\n",
        "\n",
        "so that $z$ can capture low-dimensional factors of variation and make generation and inference more manageable.\n",
        "\n",
        "---\n",
        "\n",
        "2. **What are the complementary roles of the encoder $q_\\phi(z \\mid x)$ and the decoder $p_\\theta(x \\mid z)$?**\n",
        "\n",
        "- The decoder $p_\\theta(x \\mid z)$ is the generative model: given a latent $z$, it defines a distribution over data and is used for generation and reconstruction.  \n",
        "- The encoder $q_\\phi(z \\mid x)$ is the inference model: given a data point $x$, it approximates the intractable posterior $p_\\theta(z \\mid x)$ and tells us which regions of latent space are relevant for that $x$.\n",
        "\n",
        "Together, they define an amortized variational inference scheme: one encoder network is reused across all data points.\n",
        "\n",
        "---\n",
        "\n",
        "3. **In the ELBO, what do the two terms encourage?**\n",
        "\n",
        "For\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\phi; x)\n",
        "=\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big):\n",
        "$$\n",
        "\n",
        "- The reconstruction term $\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]$ encourages the decoder and encoder together to explain $x$ well: samples $z \\sim q_\\phi(z \\mid x)$ should decode to something close to $x$.  \n",
        "- The KL term $\\mathrm{KL}(q_\\phi(z \\mid x) \\Vert p(z))$ encourages the approximate posterior to stay close to the prior: it regularizes the encoder so that latent codes live in a smooth, well-structured region that matches $p(z)$ and is usable for sampling and interpolation.\n",
        "\n",
        "---\n",
        "\n",
        "4. **If we drop the KL term entirely, what behavior should you expect?**\n",
        "\n",
        "We obtain an unregularized autoencoder:\n",
        "\n",
        "- Reconstructions of training data can be very good, because the model is free to use $z$ in any way that minimizes reconstruction loss.  \n",
        "- The latent space is no longer constrained to look like the prior. Sampling $z \\sim \\mathcal{N}(0, I)$ and decoding it typically produces poor or nonsensical samples, because the encoder has not organized codes around the prior distribution.\n",
        "\n",
        "---\n",
        "\n",
        "5. **In a $\\beta$-VAE, what happens if $\\beta$ is too large?**\n",
        "\n",
        "If the KL term is weighted too strongly relative to the reconstruction term, the encoder is heavily penalized for deviating from the prior. A typical failure mode is:\n",
        "\n",
        "- $q_\\phi(z \\mid x)$ collapses toward $p(z)$ for many $x$ (for example, $\\mu_\\phi(x) \\approx 0$ and $\\sigma_\\phi^2(x) \\approx 1$),  \n",
        "- the decoder learns to ignore $z$ and reconstruct mainly from biases or a few weak signals.\n",
        "\n",
        "This is known as posterior collapse. Reconstructions may become blurry or uninformative, and the latent space carries little useful information about $x$.\n",
        "\n",
        "---\n",
        "\n",
        "6. **In the 2D toy, when is $\\mathrm{KL}(q \\Vert p) = 0$?**\n",
        "\n",
        "In 2D with diagonal covariance we have\n",
        "$$\n",
        "\\mathrm{KL}(q \\Vert p)\n",
        "=\n",
        "\\frac{1}{2}\n",
        "\\sum_{i=1}^2\n",
        "\\big(\n",
        "\\sigma_i^2 + \\mu_i^2 - 1 - \\log \\sigma_i^2\n",
        "\\big).\n",
        "$$\n",
        "Each summand is the 1D KL and is non-negative. It equals $0$ if and only if $\\mu_i = 0$ and $\\sigma_i^2 = 1$.\n",
        "\n",
        "Therefore $\\mathrm{KL}(q \\Vert p) = 0$ if and only if $q(z)$ is exactly the prior:\n",
        "- $\\mu = (0, 0)$ and  \n",
        "- $(\\sigma_1^2, \\sigma_2^2) = (1, 1)$.\n",
        "\n",
        "In the contour plot, this is the case where the elliptical contours of $q$ match the circular contours of $p$ perfectly.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee571dca",
      "metadata": {},
      "source": [
        "### 0.6 The reparameterization trick: making the ELBO differentiable\n",
        "\n",
        "So far we have:\n",
        "\n",
        "- A generative model $p_\\theta(x \\mid z)$ with latent $z$.\n",
        "- An encoder $q_\\phi(z \\mid x)$, a diagonal Gaussian\n",
        "  $$\n",
        "  q_\\phi(z \\mid x) = \\mathcal{N}\\big(z; \\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x))\\big).\n",
        "  $$\n",
        "- An ELBO\n",
        "  $$\n",
        "  \\mathcal{L}(\\theta, \\phi; x)\n",
        "  =\n",
        "  \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "  -\n",
        "  \\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big),\n",
        "  $$\n",
        "  which we want to maximize with respect to both $\\theta$ and $\\phi$.\n",
        "\n",
        "The KL term between two diagonal Gaussians has a closed form, so its gradient is easy.  \n",
        "The non trivial part is the reconstruction term\n",
        "$$\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)],\n",
        "$$\n",
        "which involves sampling $z \\sim q_\\phi(z \\mid x)$, and depends on $\\phi$ through $q_\\phi$.\n",
        "\n",
        "Naively, if we write\n",
        "$$\n",
        "z \\sim \\mathcal{N}\\big(\\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x))\\big)\n",
        "$$\n",
        "and treat the sampling as a black box, the dependence on $\\phi$ is hidden inside the random number generator.  \n",
        "We would like a way to rewrite this expectation so that gradients with respect to $\\phi$ can pass through the sample.\n",
        "\n",
        "---\n",
        "\n",
        "#### Core idea\n",
        "\n",
        "The reparameterization trick rewrites the sampling as a deterministic function of:\n",
        "\n",
        "- simple noise with a fixed distribution, and  \n",
        "- the encoder outputs $(\\mu_\\phi(x), \\sigma_\\phi(x))$.\n",
        "\n",
        "Instead of sampling $z$ directly from $q_\\phi(z \\mid x)$, we do:\n",
        "\n",
        "1. Sample standard noise, independent of $x$ and $\\phi$:\n",
        "   $$\n",
        "   \\varepsilon \\sim \\mathcal{N}(0, I).\n",
        "   $$\n",
        "2. Transform it using the encoder outputs:\n",
        "   $$\n",
        "   z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon.\n",
        "   $$\n",
        "\n",
        "This defines exactly the same distribution for $z$:\n",
        "$$\n",
        "z \\sim \\mathcal{N}\\big(\\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x))\\big),\n",
        "$$\n",
        "but now $z$ is written as a **differentiable function** of $(\\mu_\\phi(x), \\sigma_\\phi(x))$ and of a noise variable $\\varepsilon$ that does not depend on $\\phi$.\n",
        "\n",
        "As a result, the reconstruction term can be written as\n",
        "$$\n",
        "\\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(0, I)}\n",
        "\\big[\n",
        "\\log p_\\theta\\big(x \\mid \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon\\big)\n",
        "\\big],\n",
        "$$\n",
        "and we can approximate this expectation with Monte Carlo samples of $\\varepsilon$ while still using standard backpropagation through the whole computation graph.\n",
        "\n",
        "---\n",
        "\n",
        "#### In code\n",
        "\n",
        "In practice, we parameterize $\\sigma_\\phi^2(x)$ via $\\log \\sigma_\\phi^2(x)$, and implement the trick like this:\n",
        "\n",
        "```python\n",
        "# encoder outputs\n",
        "mu      = encoder_mu(x)       # shape: (batch_size, latent_dim)\n",
        "logvar  = encoder_logvar(x)   # log sigma^2\n",
        "\n",
        "# reparameterization trick\n",
        "eps = torch.randn_like(mu)        # ε ~ N(0, I)\n",
        "std = torch.exp(0.5 * logvar)     # σ = exp(0.5 * log σ²)\n",
        "z   = mu + std * eps              # z = μ + σ ⊙ ε\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "282f2792",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Reparameterization sanity check - questions\n",
        "\n",
        "1. Why do we rewrite sampling as\n",
        "   $$\n",
        "   z = \\mu + \\sigma \\odot \\varepsilon,\n",
        "   \\quad \\varepsilon \\sim \\mathcal{N}(0, I),\n",
        "   $$\n",
        "   instead of sampling $z \\sim \\mathcal{N}(\\mu, \\mathrm{diag}(\\sigma^2))$ directly?\n",
        "\n",
        "2. In practice, the encoder predicts `log_var` (or `logvar`) instead of the variance $\\sigma^2$ directly.  \n",
        "   Why is this parameterization useful in a neural network?\n",
        "\n",
        "3. How does the choice of latent dimension $d_z$ affect what you can visualize and how you debug the model?\n",
        "\n",
        "4. Conceptually, what would go wrong if the noise $\\varepsilon$ depended on $\\phi$ instead of being sampled from a fixed distribution?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "566c4167",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### Answers:\n",
        "\n",
        "1. **Why rewrite sampling as $z = \\mu + \\sigma \\odot \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, I)$?**\n",
        "\n",
        "Both views define the same distribution for $z$:\n",
        "$$\n",
        "z \\sim \\mathcal{N}(\\mu, \\mathrm{diag}(\\sigma^2)).\n",
        "$$\n",
        "The reparameterized form\n",
        "$$\n",
        "z = \\mu + \\sigma \\odot \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, I)\n",
        "$$\n",
        "has an important advantage: the randomness is carried only by $\\varepsilon$, which has a fixed distribution that does not depend on the parameters. The map\n",
        "$$\n",
        "(\\mu, \\sigma, \\varepsilon) \\mapsto z\n",
        "$$\n",
        "is a deterministic and differentiable function of $\\mu$ and $\\sigma$. This lets gradients of the reconstruction term flow back through $z$ to the encoder parameters $\\phi$ using standard backpropagation, while still producing valid Monte Carlo samples.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Why predict `log_var` instead of $\\sigma^2$ directly?**\n",
        "\n",
        "If we predict $\\sigma^2$ directly, the network output must be constrained to be positive. This is inconvenient and numerically fragile. Predicting\n",
        "$$\n",
        "\\text{logvar}(x) = \\log \\sigma^2(x)\n",
        "$$\n",
        "instead has two advantages:\n",
        "\n",
        "- The network output can be any real number (no constraints in the last layer).  \n",
        "- We can recover a positive variance with\n",
        "  $$\n",
        "  \\sigma^2(x) = \\exp(\\text{logvar}(x)),\n",
        "  $$\n",
        "  and the corresponding standard deviation with\n",
        "  $$\n",
        "  \\sigma(x) = \\exp\\big(0.5 \\cdot \\text{logvar}(x)\\big).\n",
        "  $$\n",
        "\n",
        "This parameterization matches what you typically see in VAE implementations and tends to be numerically stable.\n",
        "\n",
        "---\n",
        "\n",
        "3. **How does the choice of latent dimension $d_z$ affect visualization and debugging?**\n",
        "\n",
        "- For $d_z = 2$, you can visualize $z$ directly with scatter plots and contour plots, and compare samples from $q_\\phi(z \\mid x)$ and $p(z)$ in the latent plane. This is very useful for building intuition.  \n",
        "- For larger $d_z$ (for example 10, 20, 50), you often get better reconstructions and more expressive latents, but you lose direct geometric intuition. You then rely on indirect diagnostics, such as:\n",
        "  - interpolations in latent space between encoded points,  \n",
        "  - per dimension histograms of $z$ compared to the prior,  \n",
        "  - monitoring the KL term and reconstruction term during training.\n",
        "\n",
        "In practice, it is common to start with $d_z = 2$ for didactic purposes, and then increase $d_z$ once the implementation is correct.\n",
        "\n",
        "---\n",
        "\n",
        "4. **What would go wrong if $\\varepsilon$ depended on $\\phi$?**\n",
        "\n",
        "The whole point of the reparameterization trick is to separate:\n",
        "\n",
        "- a fixed noise source $\\varepsilon \\sim \\mathcal{N}(0, I)$ that does not depend on the parameters, and  \n",
        "- a differentiable transformation controlled by $(\\mu_\\phi(x), \\sigma_\\phi(x))$.\n",
        "\n",
        "If $\\varepsilon$ depended on $\\phi$, then the randomness would again be entangled with the parameters. The expectation\n",
        "$$\n",
        "\\mathbb{E}_{\\varepsilon} \\big[ \\log p_\\theta(x \\mid \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon) \\big]\n",
        "$$\n",
        "would no longer have the clean structure we use to pass gradients through the sample. In the worst case, we would be back to a situation where the sampling step behaves like a black box from the point of view of gradient computation, which is exactly what the reparameterization trick avoids.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4336b30a",
      "metadata": {},
      "source": [
        "### 0.7 Toy example: from simple latent to complex data (optional)\n",
        "\n",
        "Up to now, we have focused on the **latent space**:\n",
        "\n",
        "- we chose a simple prior $p(z) = \\mathcal{N}(0, I)$,\n",
        "- we saw how the KL term encourages $q_\\phi(z \\mid x)$ to stay close to this prior,\n",
        "- and we used 2D visualizations to understand the shape of $p(z)$ and $q_\\phi(z \\mid x)$.\n",
        "\n",
        "Now we look at how a **decoder** can turn a simple latent distribution into a more complex **data distribution**.\n",
        "\n",
        "In a VAE, the decoder is a neural network $f_\\theta$ that defines (part of) $p_\\theta(x \\mid z)$.\n",
        "If we ignore observation noise for a moment and just look at the mean, we can think of it as a deterministic map\n",
        "$$\n",
        "x = f_\\theta(z).\n",
        "$$\n",
        "\n",
        "The generative story is then:\n",
        "1. sample $z \\sim p(z)$ (simple, e.g. standard Gaussian),\n",
        "2. map $x = f_\\theta(z)$,\n",
        "3. the distribution of $x$ should approximate the data distribution $p_{\\text{data}}(x)$.\n",
        "\n",
        "In this toy example, we do not train anything. Instead, we:\n",
        "\n",
        "- sample $z \\sim \\mathcal{N}(0, I_2)$ in 2D,\n",
        "- define a simple, hand-crafted decoder $f_\\alpha$,\n",
        "- visualize both $z$ and $x = f_\\alpha(z)$ side by side,\n",
        "- use a slider to control how nonlinear $f_\\alpha$ is.\n",
        "\n",
        "The key idea:\n",
        "\n",
        "- when $\\alpha = 0$, $f_\\alpha$ is close to the identity and the two plots look similar,\n",
        "- as $\\alpha$ grows, the distribution in $x$ becomes more curved and multimodal,\n",
        "  even though $z$ still follows a simple standard normal.\n",
        "\n",
        "This is exactly what we want from a VAE decoder: a flexible map that can push forward a simple $p(z)$ to a complicated $p_\\theta(x)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3b83a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, FloatSlider\n",
        "\n",
        "# Fixed latent samples z ~ N(0, I_2)\n",
        "rng = np.random.default_rng(0)\n",
        "n_points = 2000\n",
        "z = rng.standard_normal(size=(n_points, 2))  # latent space samples\n",
        "\n",
        "\n",
        "def decoder_f(z, alpha):\n",
        "    \"\"\"\n",
        "    Hand-crafted \"decoder\" f_alpha: R^2 -> R^2.\n",
        "\n",
        "    We keep x1 close to z1 and bend x2 as a nonlinear function of z1:\n",
        "\n",
        "        x1 = z1\n",
        "        x2 = z2 + alpha * (z1^2 - 1)\n",
        "\n",
        "    - For alpha = 0, x ~ N(0, I_2) (up to sampling noise).\n",
        "    - For larger alpha, the distribution of x becomes curved (banana-shaped),\n",
        "      even though z is still standard normal.\n",
        "    \"\"\"\n",
        "    x1 = z[:, 0]\n",
        "    x2 = z[:, 1] + alpha * (z[:, 0]**2 - 1.0)\n",
        "    return np.stack([x1, x2], axis=1)\n",
        "\n",
        "\n",
        "def plot_latent_and_data(alpha=0.0):\n",
        "    x = decoder_f(z, alpha)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(9, 5))\n",
        "\n",
        "    # Data space x\n",
        "    axes[0].scatter(x[:, 0], x[:, 1], s=5, alpha=0.5)\n",
        "    axes[0].set_title(r\"Data space $x = f_\\alpha(z)$\")\n",
        "    axes[0].set_xlabel(r\"$x_1$\")\n",
        "    axes[0].set_ylabel(r\"$x_2$\")\n",
        "    axes[0].grid(alpha=0.2)\n",
        "\n",
        "    # Latent space z\n",
        "    axes[1].scatter(z[:, 0], z[:, 1], s=5, alpha=0.5)\n",
        "    axes[1].set_title(r\"Latent space $z \\sim \\mathcal{N}(0, I)$\")\n",
        "    axes[1].set_xlabel(r\"$z_1$\")\n",
        "    axes[1].set_ylabel(r\"$z_2$\")\n",
        "    axes[1].grid(alpha=0.2)\n",
        "\n",
        "    fig.suptitle(rf\"Nonlinearity strength $\\alpha = {alpha:.2f}$\", y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "interact(\n",
        "    plot_latent_and_data,\n",
        "    alpha=FloatSlider(\n",
        "        value=0.0,\n",
        "        min=0.0,\n",
        "        max=2.0,\n",
        "        step=0.1,\n",
        "        description=\"alpha\",\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0fe933e",
      "metadata": {},
      "source": [
        "## Section 1 - Setup and dataset\n",
        "\n",
        "In this section you will set up the practical environment for the VAE: import the required libraries, fix random seeds, choose the compute device, and load a dataset from `torchvision` (for example MNIST, Fashion-MNIST, KMNIST, or CIFAR10). We will also inspect basic properties of the data, such as image shape and pixel range, and visualize a small batch of samples.\n",
        "\n",
        "By the end of this section you should know exactly what kind of tensors will be fed into the encoder and decoder (shape, dimensionality, and value range). This will guide the design of the VAE architecture in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dbad6cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "try:\n",
        "    from torchvision import datasets, transforms, utils as vutils\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        \"torchvision is required for this notebook. \"\n",
        "        \"Please install it with `pip install torchvision`.\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Set random seeds for reproducibility.\n",
        "\n",
        "    This does not make everything perfectly deterministic\n",
        "    (e.g. cuDNN kernels), but it reduces variability.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f045a3f",
      "metadata": {},
      "source": [
        "### 1.1 Configuration and dataset choice\n",
        "\n",
        "We start with a simple configuration:\n",
        "\n",
        "- minibatch size and number of epochs small enough to converge quickly,\n",
        "- latent dimension `LATENT_DIM = 2` for easy visualization,\n",
        "- a fully-connected encoder/decoder that flatten the images.\n",
        "\n",
        "You can later change:\n",
        "- `DATASET_NAME` to explore a different dataset,\n",
        "- `LATENT_DIM` and `HIDDEN_DIM` to increase model capacity,\n",
        "- `NUM_EPOCHS` and `LEARNING_RATE` to improve training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03837553",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supported datasets for this notebook.\n",
        "# All of them are available in torchvision.datasets.\n",
        "#\n",
        "# - \"MNIST\": handwritten digits, grayscale 28x28\n",
        "# - \"FashionMNIST\": clothing items, grayscale 28x28\n",
        "# - \"KMNIST\": Kuzushiji characters, grayscale 28x28\n",
        "# - \"CIFAR10\": small natural images, RGB 32x32 (more challenging for this simple VAE)\n",
        "# \n",
        "# You can easily add more datasets from torchvision.datasets if you like.\n",
        "# https://docs.pytorch.org/vision/main/datasets.html#image-classification\n",
        "\n",
        "SUPPORTED_DATASETS = {\"MNIST\", \"FashionMNIST\", \"KMNIST\", \"CIFAR10\"}\n",
        "\n",
        "DATASET_NAME = \"MNIST\"  # you can change this to any of the supported names above\n",
        "\n",
        "if DATASET_NAME not in SUPPORTED_DATASETS:\n",
        "    print(f\"WARNING:\\\n",
        "          \\n\\tDataset '{DATASET_NAME}' is not on the list!\\\n",
        "          \\n\\tIf you are using a dataset from torchvision.datasets, you can ignore this message.\\\n",
        "          \\n\\tDefault supported datasets: {SUPPORTED_DATASETS}\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LATENT_DIM = 2        # 2 is nice for visualization; try 10 or 32 later\n",
        "HIDDEN_DIM = 512\n",
        "NUM_EPOCHS = 5        # keep training short for class; increase at home\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "print(\n",
        "    f\"Config: dataset={DATASET_NAME}, batch_size={BATCH_SIZE}, \"\n",
        "    f\"latent_dim={LATENT_DIM}, hidden_dim={HIDDEN_DIM}, \"\n",
        "    f\"epochs={NUM_EPOCHS}, lr={LEARNING_RATE}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efaeed51",
      "metadata": {},
      "source": [
        "### 1.2 Load dataset\n",
        "\n",
        "We use datasets from [`torchvision.datasets`](https://docs.pytorch.org/vision/main/datasets.html#image-classification) :\n",
        "\n",
        "Important design choice:\n",
        "\n",
        "- We convert images to tensors in $[0, 1]$ using `transforms.ToTensor()`.\n",
        "- We do **not** normalize to zero mean / unit variance here, because in this notebook\n",
        "  we will start with a simple Bernoulli or Gaussian likelihood on pixel intensities\n",
        "  that expects inputs in $[0, 1]$.\n",
        "\n",
        "Later, we will flatten images to vectors before feeding them to the MLP encoder and decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b4d33f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_datasets(name: str, root: str = \"./data\"):\n",
        "    if name == \"MNIST\":\n",
        "        dataset_cls = datasets.MNIST\n",
        "    else:\n",
        "        # Try to resolve any other torchvision dataset by name\n",
        "        # e.g., \"Flowers102\" -> datasets.Flowers102\n",
        "        try:\n",
        "            print(\"Name:\", name)\n",
        "            dataset_cls = getattr(datasets, name)\n",
        "        except AttributeError:\n",
        "            raise ValueError(f\"Unsupported dataset: {name}\") from None\n",
        "\n",
        "    # For this notebook we keep a simple transform: just convert to tensor in [0, 1].\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),  # shape (C, H, W), values in [0, 1]\n",
        "    ])\n",
        "\n",
        "    train_ds = dataset_cls(root=root, train=True, download=True, transform=transform)\n",
        "    test_ds = dataset_cls(root=root, train=False, download=True, transform=transform)\n",
        "\n",
        "    return train_ds, test_ds\n",
        "\n",
        "\n",
        "train_dataset, test_dataset = get_datasets(DATASET_NAME)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "511f738d",
      "metadata": {},
      "source": [
        "### 1.3 Inspect shapes and pixel ranges\n",
        "\n",
        "Before we define the encoder and decoder, we need to know:\n",
        "\n",
        "- the input shape $(C, H, W)$,\n",
        "- the flattened dimensionality $D = C \\times H \\times W$,\n",
        "- the value range (here $[0, 1]$).\n",
        "\n",
        "This will determine the input/output sizes of our MLPs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c23a496",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grab one minibatch\n",
        "example_batch, example_labels = next(iter(train_loader))\n",
        "print(f\"Batch shape: {None}\") # TODO        # (batch_size, C, H, W) \n",
        "print(f\"Batch dtype: {None}\") # TODO\n",
        "print(f\"Min pixel value: {None}\") # TODO\n",
        "print(f\"Max pixel value: {None}\") # TODO\n",
        "\n",
        "C, H, W = None # TODO\n",
        "INPUT_DIM = C * H * W\n",
        "\n",
        "print(f\"Image shape: C={C}, H={H}, W={W}\")\n",
        "print(f\"Flattened input dimension D = {INPUT_DIM}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa8ac03",
      "metadata": {},
      "source": [
        "### 1.4 Visual sanity check\n",
        "\n",
        "Always look at your data.\n",
        "\n",
        "In the cell below, we plot a small grid of training images from the chosen dataset.  \n",
        "Make sure they look sensible and match your expectations:\n",
        "\n",
        "- Are they grayscale or RGB?\n",
        "- Are they roughly centered and in the expected orientation?\n",
        "- Does the content match the dataset you think you loaded?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253fa636",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to show a grid of images from a batch\n",
        "def show_batch(images, nrow=8, title=None):\n",
        "    \"\"\"\n",
        "    images: tensor of shape (B, C, H, W) with values in [0, 1]\n",
        "    \"\"\"\n",
        "    grid = vutils.make_grid(images[: nrow * nrow], nrow=nrow, padding=2)\n",
        "    # move to CPU and convert to numpy for matplotlib\n",
        "    npimg = grid.cpu().numpy()\n",
        "    # transpose from (C, H, W) to (H, W, C)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "show_batch(example_batch, nrow=8, title=f\"Samples from {DATASET_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83eb1cf",
      "metadata": {},
      "source": [
        "### Dataset sanity check - questions\n",
        "\n",
        "1. For the current value of `DATASET_NAME`, what is the image shape $(C, H, W)$?  \n",
        "   What is the flattened input dimension $D = C \\times H \\times W$ that will be used by the encoder?\n",
        "\n",
        "2. Look at the printed `Min pixel value` and `Max pixel value`.  \n",
        "   In what range do the pixel values lie, and how does this connect to the likelihood $p_\\theta(x \\mid z)$\n",
        "   we will use later (Bernoulli vs Gaussian)?\n",
        "\n",
        "3. If you switch from `MNIST` to `CIFAR10`, what changes in:\n",
        "   - the input dimension $D$,\n",
        "   - the number of channels $C$,\n",
        "   - the difficulty of the reconstruction task for a simple MLP-based VAE?\n",
        "\n",
        "4. Why might it be useful to start with `LATENT_DIM = 2` on these datasets before moving to higher dimensions?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3db856c",
      "metadata": {},
      "source": [
        "### Answers\n",
        "\n",
        "1. The image shape $(C, H, W)$ depends on `DATASET_NAME`:\n",
        "   - MNIST / FashionMNIST / KMNIST: typically $C = 1$, $H = 28$, $W = 28$, so $D = 1 \\times 28 \\times 28 = 784$.\n",
        "   - CIFAR10: $C = 3$, $H = 32$, $W = 32$, so $D = 3 \\times 32 \\times 32 = 3072$.\n",
        "\n",
        "   The encoder and decoder MLPs will take vectors in $\\mathbb{R}^D$ as input, so their first and last linear layers must be sized accordingly.\n",
        "\n",
        "2. With `transforms.ToTensor()`, pixel values lie in $[0, 1]$.  \n",
        "   This is compatible with:\n",
        "   - a Bernoulli likelihood on binarized pixels (or on $[0, 1]$ values using a cross-entropy-like loss),\n",
        "   - or a simple Gaussian likelihood with fixed variance and a mean constrained to $[0, 1]$ via a sigmoid.\n",
        "\n",
        "   In any case, knowing the range is important to choose an appropriate reconstruction loss.\n",
        "\n",
        "3. When switching to `CIFAR10`:\n",
        "   - $D$ increases from $784$ to $3072$, and the number of channels increases from $1$ to $3$.\n",
        "   - The data is RGB natural images instead of binary-like digits or clothing silhouettes.\n",
        "   - A simple MLP-based VAE has to model more complex structure and color; reconstructions will typically be much worse without using convolutional architectures and larger models.\n",
        "\n",
        "4. With $d_z = 2$, we can:\n",
        "   - visualize the latent codes directly in a 2D scatter plot,\n",
        "   - compare the empirical distribution of $q_\\phi(z \\mid x)$ to the prior $p(z)$,\n",
        "   - understand interpolation and clustering structure in the latent space.\n",
        "\n",
        "   Once the implementation is correct and intuition is built, we can increase $d_z$ to improve expressiveness and reconstruction quality, even though direct visualization is no longer possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f99a541c",
      "metadata": {},
      "source": [
        "## Section 2 - Encoder, decoder, and VAE wrapper\n",
        "\n",
        "In this section we build the neural part of the VAE:\n",
        "\n",
        "- an **encoder** $q_\\phi(z \\mid x)$ that maps images to Gaussian parameters $(\\mu_\\phi(x), \\log \\sigma_\\phi^2(x))$,\n",
        "- a **decoder** $p_\\theta(x \\mid z)$ that maps latent codes $z$ back to image space,\n",
        "- a small **VAE wrapper** that uses the encoder, applies the reparameterization trick to sample $z$, and then calls the decoder.\n",
        "\n",
        "For simplicity, we start with fully connected networks:\n",
        "we flatten each image to a vector in $\\mathbb{R}^D$ (where $D = \\text{INPUT\\_DIM}$ from Section 1),\n",
        "apply one hidden layer of size `HIDDEN_DIM`, and map to a latent space of dimension `LATENT_DIM`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ec60b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int):\n",
        "        \"\"\"\n",
        "        Simple MLP encoder that parameterizes q(z | x) as a diagonal Gaussian.\n",
        "\n",
        "        - Input:  x in R^D (flattened image) or a batch of images of shape (B, C, H, W)\n",
        "        - Output: mu(x), logvar(x) in R^{latent_dim}\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(None, None) # TODO\n",
        "        self.fc_mu = nn.Linear(None, None) # TODO\n",
        "        self.fc_logvar = nn.Linear(None, None) # TODO\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: tensor of shape (B, C, H, W) or (B, D).\n",
        "\n",
        "        Returns:\n",
        "            mu:     (B, latent_dim)\n",
        "            logvar: (B, latent_dim)  where logvar = log sigma^2\n",
        "        \"\"\"\n",
        "        # Flatten images to vectors of dimension D = C * H * W if needed\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Shared hidden representation h(x)\n",
        "        h = self.activation(self.fc1(x))\n",
        "\n",
        "        # Mean and log-variance of q(z | x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a417612d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim: int, hidden_dim: int, output_dim: int):\n",
        "        \"\"\"\n",
        "        Simple MLP decoder that parameterizes p(x | z).\n",
        "\n",
        "        - Input:  z in R^{latent_dim}\n",
        "        - Output: reconstruction in R^D (flattened image logits or means)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(None, None) # TODO\n",
        "        self.fc_out = nn.Linear(None, None) # TODO\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: tensor of shape (B, latent_dim)\n",
        "\n",
        "        Returns:\n",
        "            recon: tensor of shape (B, output_dim)\n",
        "                   later we may interpret this as:\n",
        "                   - Bernoulli logits per pixel, or\n",
        "                   - Gaussian means per pixel (after a sigmoid).\n",
        "        \"\"\"\n",
        "        h = self.activation(self.fc1(z))\n",
        "        recon = self.fc_out(h)\n",
        "        return recon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14dcddf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reparameterization trick:\n",
        "\n",
        "        z = mu + sigma ⊙ eps,     eps ~ N(0, I)\n",
        "\n",
        "    where:\n",
        "        - mu     has shape (B, latent_dim)\n",
        "        - logvar has shape (B, latent_dim), logvar = log sigma^2\n",
        "\n",
        "    This makes z a differentiable function of (mu, logvar) and random noise eps.\n",
        "    \"\"\"\n",
        "    std = None # TODO                   # sigma = exp(0.5 * log sigma^2)\n",
        "    eps = torch.randn_like(std)         # eps ~ N(0, I)\n",
        "    z = mu + std * eps                  # elementwise multiplication\n",
        "    return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0701db0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This one will be useful later\n",
        "def collect_latent_snapshot(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    max_batches: int = 50,\n",
        "    max_points: int = 5000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Collect a snapshot of the latent means mu(x) and labels y for visualization.\n",
        "\n",
        "    - Only defined for LATENT_DIM = 2 (we project mu directly).\n",
        "    - Iterates over at most `max_batches` minibatches.\n",
        "    - Truncates to at most `max_points` points to keep the animation light.\n",
        "\n",
        "    Returns:\n",
        "    - mu_all:     tensor of shape (N, 2)\n",
        "    - labels_all: tensor of shape (N,)\n",
        "    \"\"\"\n",
        "    if LATENT_DIM != 2:\n",
        "        raise ValueError(\"Latent snapshots for animation are only supported when LATENT_DIM = 2.\")\n",
        "\n",
        "    model.eval()\n",
        "    all_mu = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(data_loader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            mu, logvar = model.encoder(x)\n",
        "            all_mu.append(mu)\n",
        "            all_labels.append(y)\n",
        "\n",
        "            if batch_idx + 1 >= max_batches:\n",
        "                break\n",
        "\n",
        "    mu_all = torch.cat(all_mu, dim=0)\n",
        "    labels_all = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Truncate if too many points\n",
        "    if mu_all.size(0) > max_points:\n",
        "        mu_all = mu_all[:max_points]\n",
        "        labels_all = labels_all[:max_points]\n",
        "\n",
        "    return mu_all.cpu(), labels_all.cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670076e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Full VAE forward pass.\n",
        "\n",
        "        Args:\n",
        "            x: input images, shape (B, C, H, W) or (B, D)\n",
        "\n",
        "        Returns:\n",
        "            recon:   reconstructed images, shape (B, D)\n",
        "            mu:      mean of q(z | x), shape (B, latent_dim)\n",
        "            logvar:  log-variance of q(z | x), shape (B, latent_dim)\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = None # reparameterization trick\n",
        "        recon = self.decoder(z)\n",
        "        return recon, mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe313f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VAE(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88f1d744",
      "metadata": {},
      "source": [
        "### 2.1 Shape sanity check\n",
        "\n",
        "Before writing the loss function, verify that:\n",
        "\n",
        "- the encoder produces $(\\mu, \\log \\sigma^2)$ with the expected shapes,\n",
        "- the reparameterization trick produces $z$ with the correct shape,\n",
        "- the decoder returns a reconstruction with the same flattened size $D$ as the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e7a48d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_example, _ = next(iter(train_loader))\n",
        "    x_example = x_example.to(device)\n",
        "\n",
        "    recon, mu, logvar = model(x_example)\n",
        "\n",
        "print(f\"Input batch shape:       {x_example.shape}\")       # (B, C, H, W)\n",
        "print(f\"Flattened input dim D:   {INPUT_DIM}\")\n",
        "print(f\"mu shape:                {mu.shape}\")              # (B, LATENT_DIM)\n",
        "print(f\"logvar shape:            {logvar.shape}\")          # (B, LATENT_DIM)\n",
        "print(f\"Reconstruction shape:    {recon.shape}\")           # (B, D)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f984f01",
      "metadata": {},
      "source": [
        "You can also check that the reconstruction can be reshaped back to image form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf691f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "B = x_example.size(0)\n",
        "recon_images = recon.view(B, C, H, W)\n",
        "print(f\"Reconstruction reshaped to images: {recon_images.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303ca3c",
      "metadata": {},
      "source": [
        "### Encoder/decoder sanity check - questions\n",
        "\n",
        "1. In the encoder, why do we output both `mu` and `logvar` with shape `(B, LATENT_DIM)`?\n",
        "   How does this connect to the probabilistic definition of $q_\\phi(z \\mid x)$?\n",
        "\n",
        "2. Where, in the code above, is the reparameterization trick actually implemented?\n",
        "   How does that implementation correspond to the equation\n",
        "   $$\n",
        "   z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon,\n",
        "   \\quad \\varepsilon \\sim \\mathcal{N}(0, I)?\n",
        "   $$\n",
        "\n",
        "3. Why do we not use any nonlinearity after `self.fc_mu` and `self.fc_logvar` in the encoder, nor after `self.fc_out` in the decoder?\n",
        "\n",
        "4. Consider the effect of `LATENT_DIM`:\n",
        "   - What changes in the **shapes** of `mu`, `logvar`, `z`, and `recon` if you increase `LATENT_DIM` from 2 to 20?\n",
        "   - What do you expect to happen to reconstruction quality and latent visualization as you increase `LATENT_DIM`?\n",
        "\n",
        "5. What aspects of this architecture make it clearly suboptimal for datasets like CIFAR10, and what kind of changes would you consider if you wanted better reconstructions on natural images?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869d3044",
      "metadata": {},
      "source": [
        "### Encoder/decoder sanity check - answers\n",
        "\n",
        "1. **Why do we output both `mu` and `logvar` with shape `(B, LATENT_DIM)`?**\n",
        "\n",
        "   The encoder parameterizes $q_\\phi(z \\mid x)$ as a diagonal Gaussian\n",
        "   $$\n",
        "   q_\\phi(z \\mid x) = \\mathcal{N}\\big(z; \\mu_\\phi(x), \\mathrm{diag}(\\sigma_\\phi^2(x))\\big).\n",
        "   $$\n",
        "   For each input $x$ (or each element in a minibatch), we need:\n",
        "   - a mean vector $\\mu_\\phi(x)$ in $\\mathbb{R}^{d_z}$,\n",
        "   - and a variance vector $\\sigma_\\phi^2(x)$ in $\\mathbb{R}^{d_z}$.\n",
        "\n",
        "   We represent the variance via `logvar = log sigma^2`, so both `mu` and `logvar` have shape `(B, LATENT_DIM)`. This gives us all the parameters needed to define $q_\\phi(z \\mid x)$ for each example in the batch.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Where is the reparameterization trick implemented, and how does it match the equation?**\n",
        "\n",
        "   The reparameterization trick is implemented in the function `reparameterize`:\n",
        "\n",
        "       std = torch.exp(0.5 * logvar)\n",
        "       eps = torch.randn_like(std)\n",
        "       z   = mu + std * eps\n",
        "\n",
        "   This corresponds exactly to the equation\n",
        "\n",
        "   $$\n",
        "   z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\varepsilon,\n",
        "   \\quad \\varepsilon \\sim \\mathcal{N}(0, I),\n",
        "   $$\n",
        "\n",
        "   where:\n",
        "   - `std` represents $\\sigma_\\phi(x)$,\n",
        "   - `eps` represents $\\varepsilon$,\n",
        "   - and `z` is the reparameterized latent variable.\n",
        "\n",
        "   The key point is that $\\varepsilon$ has a fixed distribution that does not depend on the encoder parameters, while $z$ is a differentiable function of $(\\mu_\\phi(x), \\sigma_\\phi(x))$ and $\\varepsilon$.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Why no nonlinearity after `fc_mu`, `fc_logvar` in the encoder, or after `fc_out` in the decoder?**\n",
        "\n",
        "   For the encoder:\n",
        "   - $\\mu_\\phi(x)$ is allowed to be any real vector in $\\mathbb{R}^{d_z}$, so we do not want to constrain its range with a nonlinearity.\n",
        "   - `logvar` is also allowed to be any real vector, since the variance is recovered as\n",
        "     $$\n",
        "     \\sigma^2 = \\exp(\\text{logvar}) > 0.\n",
        "     $$\n",
        "     A nonlinearity on `logvar` would complicate this relationship unnecessarily.\n",
        "\n",
        "   For the decoder:\n",
        "   - `fc_out` produces raw values in $\\mathbb{R}^D$. Depending on our choice of likelihood $p_\\theta(x \\mid z)$, we will apply an appropriate nonlinearity inside the loss:\n",
        "     - for a Bernoulli likelihood on pixels in $[0, 1]$, we will typically apply a sigmoid and then use a binary cross-entropy style loss,\n",
        "     - for a Gaussian likelihood with fixed variance, we might treat the raw outputs as means and combine them with a mean squared error.\n",
        "\n",
        "   Keeping the decoder output linear makes the connection between the network output and the likelihood explicit in the loss function, rather than embedding it inside the network architecture.\n",
        "\n",
        "---\n",
        "\n",
        "4. **What changes when we increase `LATENT_DIM`, and what do we expect qualitatively?**\n",
        "\n",
        "   If we increase `LATENT_DIM` from 2 to 20:\n",
        "\n",
        "   - Shapes:\n",
        "     - `mu` and `logvar` change from `(B, 2)` to `(B, 20)`,\n",
        "     - `z` also changes from `(B, 2)` to `(B, 20)`,\n",
        "     - `recon` remains `(B, D)` because the output dimension is fixed by the image size.\n",
        "\n",
        "   - Behavior:\n",
        "     - A larger latent dimension typically increases the capacity of the model and can improve reconstruction quality, because the encoder has more degrees of freedom to encode information about $x$.\n",
        "     - However, we lose the ability to directly visualize the latent space as a simple scatter plot, which is very convenient when $d_z = 2$. For larger $d_z$, we must rely on indirect diagnostics such as interpolations, per-dimension statistics, and the training curves of reconstruction and KL terms.\n",
        "\n",
        "---\n",
        "\n",
        "5. **Why is this MLP architecture not ideal for CIFAR10-like data, and what would you change?**\n",
        "\n",
        "   CIFAR10 images are small natural images with:\n",
        "   - three color channels (RGB),\n",
        "   - rich local structure (edges, textures, objects),\n",
        "   - and more complex global patterns than digits or simple shapes.\n",
        "\n",
        "   A fully connected MLP:\n",
        "   - treats all pixels as independent coordinates in a flat vector,\n",
        "   - ignores spatial locality and translation invariance,\n",
        "   - struggles to capture fine-grained structure, often leading to blurry or unrealistic reconstructions.\n",
        "\n",
        "   For better performance on such data, we would typically:\n",
        "   - replace the MLP with **convolutional encoders and decoders** that exploit spatial structure,\n",
        "   - use deeper networks with more channels and nonlinearities,\n",
        "   - possibly adopt more expressive decoders (e.g. deconvolutional layers, residual blocks).\n",
        "\n",
        "   In this notebook we deliberately keep the architecture simple (MLP + flattened images) to focus on the VAE concepts, but for serious image modeling, convolutional VAEs are much more appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8602ff23",
      "metadata": {},
      "source": [
        "## Section 3 - VAE loss: reconstruction + KL\n",
        "\n",
        "We now translate the ELBO into a concrete loss function.\n",
        "\n",
        "From Section 0, for a single data point $x$ we had the ELBO\n",
        "$$\n",
        "\\mathcal{L}(\\theta, \\phi; x)\n",
        "=\n",
        "\\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "-\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big).\n",
        "$$\n",
        "\n",
        "In code we usually **minimize** the negative ELBO:\n",
        "$$\n",
        "\\mathcal{L}_{\\text{VAE}}(x)\n",
        "=\n",
        "- \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "+\n",
        "\\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big),\n",
        "$$\n",
        "which decomposes into:\n",
        "\n",
        "- a **reconstruction loss** (negative log-likelihood), and  \n",
        "- a **KL penalty** between the approximate posterior and the prior.\n",
        "\n",
        "In this section we implement:\n",
        "\n",
        "1. a reconstruction loss based on binary cross-entropy with logits,  \n",
        "2. a closed form KL for diagonal Gaussians,  \n",
        "3. a `vae_loss` function that combines them, with an optional $\\beta$ factor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4793d61a",
      "metadata": {},
      "source": [
        "You can later swap this out for an MSE-based loss if you want a Gaussian likelihood instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467655f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruction_loss(recon_logits: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reconstruction loss for the VAE.\n",
        "\n",
        "    We treat each pixel as a Bernoulli variable and use\n",
        "    binary cross-entropy with logits (BCEWithLogits).\n",
        "\n",
        "    Assumptions:\n",
        "    - recon_logits: raw decoder outputs of shape (B, D),\n",
        "                    interpreted as Bernoulli logits per pixel.\n",
        "    - x: input images, either (B, C, H, W) or (B, D), with values in [0, 1].\n",
        "\n",
        "    Returns:\n",
        "    - scalar tensor (average over batch).\n",
        "    \"\"\"\n",
        "    # Flatten x to (B, D) if it has spatial dimensions\n",
        "    if x.dim() > 2:\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "    # BCE with logits: stable implementation of\n",
        "    #   -log p_theta(x | z) for a factorized Bernoulli likelihood.\n",
        "    \n",
        "    \n",
        "    # TODO: Search for 'binary cross-entropy with logits' from the\n",
        "    #       PyTorch documentation and use it here.\n",
        "    # Tip: reduction=\"none\" keeps per-pixel losses, use it.\n",
        "    bce = None # TODO\n",
        "\n",
        "    # Sum over features D, then average over batch\n",
        "    recon = bce.sum(dim=1).mean()\n",
        "    return recon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676c28d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def kl_divergence(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    KL(q(z | x) || p(z)) for diagonal Gaussian q and standard normal p.\n",
        "\n",
        "    q(z | x) = N(z; mu, diag(sigma^2))\n",
        "    p(z)     = N(z; 0, I)\n",
        "\n",
        "    For each latent dimension i, the closed form is:\n",
        "        KL_i = 0.5 * (sigma_i^2 + mu_i^2 - 1 - log sigma_i^2)\n",
        "             = -0.5 * (1 + logvar_i - mu_i^2 - exp(logvar_i))\n",
        "\n",
        "    We sum over latent dimensions and average over the batch.\n",
        "    \"\"\"\n",
        "    # Convert log-variance to variance\n",
        "    # logvar has shape (B, latent_dim)\n",
        "    sigma2 = torch.exp(logvar)\n",
        "\n",
        "    # Using the equivalent standard formula:\n",
        "    # KL = 0.5 * (sigma2 + mu^2 - 1 - logvar)\n",
        "    kl_per_dim = 0.5 * (sigma2 + mu.pow(2) - 1.0 - logvar)\n",
        "\n",
        "    # Sum over latent dimensions, mean over batch\n",
        "    kl = kl_per_dim.sum(dim=1).mean()\n",
        "    return kl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1d63346",
      "metadata": {},
      "source": [
        "**Note:** this is the same formula you used in the 1D and 2D KL toys, just vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e95757d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def vae_loss(\n",
        "    recon_logits: torch.Tensor,\n",
        "    x: torch.Tensor,\n",
        "    mu: torch.Tensor,\n",
        "    logvar: torch.Tensor,\n",
        "    beta: float = 1.0,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Compute the VAE loss for a batch:\n",
        "\n",
        "        loss = recon + beta * KL(q(z | x) || p(z))\n",
        "\n",
        "    where:\n",
        "    - recon is the reconstruction loss (negative log-likelihood),\n",
        "    - KL is the regularization term from the ELBO,\n",
        "    - beta = 1.0 recovers the standard VAE objective,\n",
        "      and beta != 1.0 yields a beta-VAE variant.\n",
        "\n",
        "    Returns:\n",
        "    - loss:  scalar tensor\n",
        "    - recon: scalar tensor (reconstruction term)\n",
        "    - kl:    scalar tensor (KL term)\n",
        "    \"\"\"\n",
        "    recon = None # TODO\n",
        "    kl =  None # TODO\n",
        "    loss = None # TODO\n",
        "    return loss, recon, kl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fe6285",
      "metadata": {},
      "source": [
        "Note: This function is what you will call inside the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df000725",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity check: mean and std of reparameterized samples\n",
        "with torch.no_grad():\n",
        "    mu_test = torch.tensor([[1.0, -1.0]], device=device)           # shape (1, 2)\n",
        "    logvar_test = torch.log(torch.tensor([[0.5, 1.5]], device=device))  # log sigma^2\n",
        "\n",
        "    # Repeat parameters to get many samples\n",
        "    mu_rep = mu_test.expand(4000, -1)\n",
        "    logvar_rep = logvar_test.expand(4000, -1)\n",
        "\n",
        "    samples = reparameterize(mu_rep, logvar_rep)\n",
        "\n",
        "    empirical_mean = samples.mean(dim=0)\n",
        "    empirical_std = samples.std(dim=0)\n",
        "    target_std = torch.exp(0.5 * logvar_test.squeeze(0))\n",
        "\n",
        "print(\"Target mu:    \", mu_test.squeeze(0).cpu().numpy())\n",
        "print(\"Empirical mu: \", empirical_mean.cpu().numpy())\n",
        "print(\"Target std:   \", target_std.cpu().numpy())\n",
        "print(\"Empirical std:\", empirical_std.cpu().numpy())\n",
        "\n",
        "# Check KL magnitude for these parameters\n",
        "with torch.no_grad():\n",
        "    kl_test = kl_divergence(mu_test, logvar_test)\n",
        "print(f\"KL(q || p) for this test Gaussian: {kl_test.item():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c9ba27",
      "metadata": {},
      "source": [
        "You should see empirical mean/std close to target, and a positive KL."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5efcc7f",
      "metadata": {},
      "source": [
        "### 3.1 End-to-end shape sanity check\n",
        "\n",
        "Before writing the training loop, verify that:\n",
        "\n",
        "- `model(x)` returns tensors with the expected shapes,\n",
        "- `vae_loss` accepts them and returns scalar losses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2248efc",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_batch, _ = next(iter(train_loader))\n",
        "    x_batch = x_batch.to(device)\n",
        "\n",
        "    # Forward pass through the VAE model\n",
        "    recon_logits, mu, logvar = model(x_batch)\n",
        "\n",
        "    # Compute loss once to check it runs\n",
        "    loss, recon, kl = vae_loss(recon_logits, x_batch, mu, logvar, beta=1.0)\n",
        "\n",
        "print(f\"Input batch shape:        {x_batch.shape}\")             # (B, C, H, W)\n",
        "print(f\"Recon logits shape:       {recon_logits.shape}\")        # (B, D = INPUT_DIM)\n",
        "print(f\"mu shape:                 {mu.shape}\")                  # (B, LATENT_DIM)\n",
        "print(f\"logvar shape:             {logvar.shape}\")              # (B, LATENT_DIM)\")\n",
        "print(f\"Total loss (scalar):      {loss.item():.3f}\")\n",
        "print(f\"Recon term (scalar):      {recon.item():.3f}\")\n",
        "print(f\"KL term (scalar):         {kl.item():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c941c1bb",
      "metadata": {},
      "source": [
        "### VAE loss sanity check - questions\n",
        "\n",
        "1. Suppose we used a simple mean squared error (MSE) instead of\n",
        "   `binary_cross_entropy_with_logits` for the reconstruction term.\n",
        "   What implicit likelihood model $p_\\theta(x \\mid z)$ would that correspond to?\n",
        "   How does this differ from the Bernoulli likelihood we are using here?\n",
        "\n",
        "2. In the KL code, we used the formula\n",
        "   $$\n",
        "   \\mathrm{KL}(q \\Vert p)\n",
        "   =\n",
        "   \\frac{1}{2} \\sum_i \\big( \\sigma_i^2 + \\mu_i^2 - 1 - \\log \\sigma_i^2 \\big).\n",
        "   $$\n",
        "   Why is this always non negative, and why does it become zero if and only if\n",
        "   $q(z \\mid x)$ equals the prior $p(z)$?\n",
        "\n",
        "3. In `vae_loss`, what is the effect of choosing $\\beta \\ll 1$ or $\\beta \\gg 1$\n",
        "   on the balance between reconstruction quality and latent regularity?\n",
        "   Relate your answer to the discussion of posterior collapse.\n",
        "\n",
        "4. During training, if you see that:\n",
        "   - the reconstruction term keeps decreasing,\n",
        "   - but the KL term collapses close to zero and stays there,\n",
        "   what behavior do you suspect in the encoder and decoder?\n",
        "\n",
        "5. For a fixed $\\beta$, how would you monitor whether the model is overfitting\n",
        "   to the training set using the quantities we defined in this section?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fc52cf",
      "metadata": {},
      "source": [
        "### VAE loss sanity check - answers\n",
        "\n",
        "1. Using mean squared error (MSE) as the reconstruction term corresponds to a\n",
        "   Gaussian likelihood of the form\n",
        "   $$\n",
        "   p_\\theta(x \\mid z) = \\mathcal{N}\\big(x; \\mu_\\theta(z), \\sigma^2 I\\big),\n",
        "   $$\n",
        "   with a fixed variance $\\sigma^2$ and the decoder output interpreted as\n",
        "   the mean $\\mu_\\theta(z)$. This is different from the Bernoulli likelihood,\n",
        "   where each pixel is modeled as a Bernoulli random variable with parameter\n",
        "   given by a sigmoid-transformed decoder output. In practice, MSE often leads\n",
        "   to blurrier reconstructions on image data than a Bernoulli or more expressive\n",
        "   likelihood.\n",
        "\n",
        "2. The closed form KL divergence between two Gaussians is always non negative\n",
        "   because it is a special case of the general KL divergence, which satisfies\n",
        "   $\\mathrm{KL}(q \\Vert p) \\ge 0$ for all distributions $q, p$. In this diagonal\n",
        "   Gaussian case, each one dimensional term\n",
        "   $$\n",
        "   \\mathrm{KL}_i\n",
        "   =\n",
        "   \\frac{1}{2} \\big( \\sigma_i^2 + \\mu_i^2 - 1 - \\log \\sigma_i^2 \\big)\n",
        "   $$\n",
        "   is itself non negative and equals $0$ if and only if $\\mu_i = 0$ and\n",
        "   $\\sigma_i^2 = 1$. Therefore the full KL is zero if and only if all coordinates\n",
        "   match the prior, that is when $q(z \\mid x)$ is exactly $p(z)$.\n",
        "\n",
        "3. In\n",
        "   $$\n",
        "   \\mathcal{L}_{\\text{VAE}}(x)\n",
        "   =\n",
        "   - \\mathbb{E}_{q_\\phi(z \\mid x)}[\\log p_\\theta(x \\mid z)]\n",
        "   +\n",
        "   \\beta \\, \\mathrm{KL}\\big(q_\\phi(z \\mid x) \\Vert p(z)\\big),\n",
        "   $$\n",
        "   a small $\\beta \\ll 1$ weakens the regularization, so the model can focus on\n",
        "   reconstruction and is more likely to overfit and produce a messy latent space.\n",
        "   A large $\\beta \\gg 1$ strongly penalizes deviations from the prior, which can\n",
        "   lead to posterior collapse: the encoder outputs $q_\\phi(z \\mid x)$ that are\n",
        "   close to the prior for most $x$, and the decoder effectively ignores $z$\n",
        "   and reconstructs from biases or trivial signals.\n",
        "\n",
        "4. If the reconstruction term decreases while the KL term collapses to zero,\n",
        "   it is a strong signal of posterior collapse. The encoder is likely producing\n",
        "   $q_\\phi(z \\mid x)$ that look almost like the prior for most inputs\n",
        "   (for example, $\\mu_\\phi(x) \\approx 0$ and $\\sigma_\\phi^2(x) \\approx 1$),\n",
        "   so the latent variable $z$ carries little information about $x$.\n",
        "   The decoder then learns to reconstruct $x$ from very weak or constant latent\n",
        "   inputs, often resulting in blurry or uninformative reconstructions.\n",
        "\n",
        "5. To monitor overfitting, you can track the reconstruction and KL terms on both\n",
        "   the training and test (or validation) sets:\n",
        "   - if the training reconstruction loss keeps decreasing while the test\n",
        "     reconstruction loss starts increasing, this suggests overfitting,\n",
        "   - similarly, you can compare the KL term on train and test to see whether\n",
        "     the latent structure learned on the training set generalizes.\n",
        "   Plotting both terms (and their sum) for training and test over epochs is a\n",
        "   simple and effective diagnostic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b9fc8d",
      "metadata": {},
      "source": [
        "## Section 4 - Training loop and basic evaluation\n",
        "\n",
        "We now train the VAE using the loss from Section 3.\n",
        "\n",
        "For each minibatch $x$ we will:\n",
        "\n",
        "1. Run the model to get $(\\text{recon\\_logits}, \\mu, \\log \\sigma^2)$.\n",
        "2. Compute the VAE loss\n",
        "   $$\n",
        "   \\text{loss} = \\text{recon} + \\beta \\cdot \\text{KL},\n",
        "   $$\n",
        "   where $\\beta = 1$ by default.\n",
        "3. Backpropagate and update the encoder and decoder parameters.\n",
        "\n",
        "We will:\n",
        "- log the average loss, reconstruction term, and KL term per epoch,\n",
        "- evaluate the model on the test set after each epoch.\n",
        "\n",
        "Later, we will visualize reconstructions, samples, and the latent space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2468153",
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "print(f\"Optimizer: Adam, lr={LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d26624",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    epoch: int,\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    beta: float = 1.0,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Run one epoch of training.\n",
        "\n",
        "    Returns:\n",
        "    - avg_loss:  average total loss over the epoch\n",
        "    - avg_recon: average reconstruction term\n",
        "    - avg_kl:    average KL term\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_recon = 0.0\n",
        "    running_kl = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: encoder, reparameterization, decoder\n",
        "        recon_logits, mu, logvar =  None # TODO\n",
        "\n",
        "        # VAE loss\n",
        "        loss, recon, kl = None # TODO\n",
        "\n",
        "        # Backward and parameter update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_recon += recon.item()\n",
        "        running_kl += kl.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Optional progress print inside the epoch\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = running_loss / num_batches\n",
        "            avg_recon = running_recon / num_batches\n",
        "            avg_kl = running_kl / num_batches\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} - Batch {batch_idx + 1:04d}/{len(dataloader):04d}  \"\n",
        "                f\"Loss: {avg_loss:.2f}  Recon: {avg_recon:.2f}  KL: {avg_kl:.2f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = running_loss / num_batches\n",
        "    avg_recon = running_recon / num_batches\n",
        "    avg_kl = running_kl / num_batches\n",
        "    return avg_loss, avg_recon, avg_kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c757ace0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    beta: float = 1.0,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate the VAE on a dataset without updating parameters.\n",
        "\n",
        "    Returns:\n",
        "    - avg_loss:  average total loss over the dataset\n",
        "    - avg_recon: average reconstruction term\n",
        "    - avg_kl:    average KL term\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_recon = 0.0\n",
        "    running_kl = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, _ in dataloader:\n",
        "            x = x.to(device)\n",
        "\n",
        "            recon_logits, mu, logvar = None # TODO\n",
        "            loss, recon, kl = None # TODO\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_recon += recon.item()\n",
        "            running_kl += kl.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = running_loss / num_batches\n",
        "    avg_recon = running_recon / num_batches\n",
        "    avg_kl = running_kl / num_batches\n",
        "    return avg_loss, avg_recon, avg_kl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "255df938",
      "metadata": {},
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f9baa8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    num_epochs: int,\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    test_loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    beta: float = 1.0,\n",
        "    latent_snapshots: list | None = None,      # list to store (mu, labels) per epoch\n",
        "    latent_loader: DataLoader | None = None,   # which loader to use for snapshots\n",
        "):\n",
        "    \"\"\"\n",
        "    Full training loop with train and test metrics per epoch.\n",
        "\n",
        "    If latent_snapshots and latent_loader are provided and LATENT_DIM == 2,\n",
        "    we will record:\n",
        "      - one latent snapshot BEFORE training starts (epoch 0),\n",
        "      - one snapshot AFTER each epoch.\n",
        "\n",
        "    Returns:\n",
        "    - history: dict with train/test losses (as before).\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_recon\": [],\n",
        "        \"train_kl\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_recon\": [],\n",
        "        \"test_kl\": [],\n",
        "    }\n",
        "\n",
        "    # Initial snapshot before any training step (epoch 0) for visualization later\n",
        "    if latent_snapshots is not None and latent_loader is not None and LATENT_DIM == 2:\n",
        "        print(\"Collecting initial latent snapshot (epoch 0, untrained model)...\")\n",
        "        mu_snapshot, labels_snapshot = collect_latent_snapshot(\n",
        "            model,\n",
        "            latent_loader,\n",
        "            max_batches=50,\n",
        "            max_points=3000,\n",
        "        )\n",
        "        latent_snapshots.append((mu_snapshot, labels_snapshot))\n",
        "\n",
        "    # Training epochs\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_recon, train_kl = train_one_epoch(\n",
        "            epoch, model, train_loader, optimizer, beta=beta\n",
        "        )\n",
        "        test_loss, test_recon, test_kl = evaluate(\n",
        "            model, test_loader, beta=beta\n",
        "        )\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_recon\"].append(train_recon)\n",
        "        history[\"train_kl\"].append(train_kl)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_recon\"].append(test_recon)\n",
        "        history[\"test_kl\"].append(test_kl)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} \"\n",
        "            f\"| train loss: {train_loss:.2f} (recon {train_recon:.2f}, KL {train_kl:.2f}) \"\n",
        "            f\"| test loss: {test_loss:.2f} (recon {test_recon:.2f}, KL {test_kl:.2f})\"\n",
        "        )\n",
        "\n",
        "        # Snapshot after this epoch\n",
        "        if latent_snapshots is not None and latent_loader is not None and LATENT_DIM == 2:\n",
        "            mu_snapshot, labels_snapshot = collect_latent_snapshot(\n",
        "                model,\n",
        "                latent_loader,\n",
        "                max_batches=50,\n",
        "                max_points=3000,\n",
        "            )\n",
        "            latent_snapshots.append((mu_snapshot, labels_snapshot))\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e751bcf",
      "metadata": {},
      "source": [
        "### 4.1 Launch training\n",
        "\n",
        "You can now train the VAE. For the default configuration, this should take\n",
        "a few minutes at most on a GPU, and longer on CPU.\n",
        "\n",
        "Make sure first that:\n",
        "\n",
        "- the shape sanity checks in Sections 2 and 3 passed,\n",
        "- there are no `nan` values in the initial loss,\n",
        "- you set `DATASET_NAME`, `LATENT_DIM`, and `NUM_EPOCHS` to reasonable values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9c0250",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to start training once you are confident that\n",
        "# reparameterization, VAE loss, and the model definitions are correct.\n",
        "\n",
        "latent_snapshots = []\n",
        "\n",
        "history = train(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    beta=1.0,\n",
        "    latent_snapshots=latent_snapshots,\n",
        "    latent_loader=test_loader,          # use test set for visualization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881f44d3",
      "metadata": {},
      "source": [
        "### 4.2 Training curves (optional)\n",
        "\n",
        "If you stored the `history` returned by `train`, you can visualize:\n",
        "\n",
        "- train vs test total loss,\n",
        "- reconstruction term,\n",
        "- KL term,\n",
        "\n",
        "as a function of the epoch. This is very helpful to detect overfitting and\n",
        "posterior collapse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7989c6e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_curves(history):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "\n",
        "    # Total loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"train\")\n",
        "    plt.plot(epochs, history[\"test_loss\"], label=\"test\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(\"Total VAE loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Reconstruction term\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, history[\"train_recon\"], label=\"train\")\n",
        "    plt.plot(epochs, history[\"test_recon\"], label=\"test\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"recon\")\n",
        "    plt.title(\"Reconstruction term\")\n",
        "    plt.legend()\n",
        "\n",
        "    # KL term\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, history[\"train_kl\"], label=\"train\")\n",
        "    plt.plot(epochs, history[\"test_kl\"], label=\"test\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"KL\")\n",
        "    plt.title(\"KL term\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# After training:\n",
        "plot_training_curves(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82943d82",
      "metadata": {},
      "source": [
        "### Training loop sanity check - questions\n",
        "\n",
        "1. In `train_one_epoch`, why do we call `optimizer.zero_grad()` before the\n",
        "   forward pass, and why do we call `loss.backward()` before `optimizer.step()`?\n",
        "\n",
        "2. Why do we use `model.train()` during training and `model.eval()` during\n",
        "   evaluation, even though we do not explicitly use dropout or batch normalization\n",
        "   in this simple VAE?\n",
        "\n",
        "3. Suppose you accidentally wrote:\n",
        "\n",
        "       loss = recon + beta * kl  \n",
        "       loss.backward()  \n",
        "       optimizer.step()  \n",
        "       optimizer.zero_grad()  \n",
        "\n",
        "   What is wrong with this order, and what effect would it have on training?\n",
        "\n",
        "4. If during training you observe that:\n",
        "   - train loss decreases,\n",
        "   - test loss decreases at first but then starts to increase,\n",
        "   what kind of behavior is this, and what adjustments might you consider?\n",
        "\n",
        "5. How could you use the recorded `train_recon`, `train_kl`, `test_recon`,\n",
        "   and `test_kl` values to diagnose posterior collapse?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75f9e85",
      "metadata": {},
      "source": [
        "### Training loop sanity check - answers\n",
        "\n",
        "1. We call `optimizer.zero_grad()` to reset any gradients that were accumulated\n",
        "   in the previous iteration. In PyTorch, gradients are accumulated by default,\n",
        "   so if we did not reset them, each new `loss.backward()` would add its\n",
        "   gradients on top of the old ones. The correct order is:\n",
        "   - zero gradients,\n",
        "   - compute forward pass and loss,\n",
        "   - compute gradients with `loss.backward()`,\n",
        "   - update parameters with `optimizer.step()`.\n",
        "\n",
        "2. `model.train()` and `model.eval()` set an internal mode flag that is used\n",
        "   by certain layers such as dropout and batch normalization. Even if we are\n",
        "   not using those layers now, it is good practice to keep this pattern so that\n",
        "   the code remains correct if we later add such layers. In more complex VAEs,\n",
        "   forgetting to switch between train and eval modes is a common source of bugs.\n",
        "\n",
        "3. If we call `optimizer.zero_grad()` after `optimizer.step()` in the wrong place,\n",
        "   for example:\n",
        "\n",
        "       loss = recon + beta * kl  \n",
        "       loss.backward()  \n",
        "       optimizer.step()  \n",
        "       optimizer.zero_grad()  \n",
        "\n",
        "   then the current iteration is still fine, but it is easy to accidentally move\n",
        "   `zero_grad()` so that gradients are cleared before they are used, or to forget\n",
        "   to call it before the next iteration. The intended pattern per iteration is:\n",
        "   - zero gradients,\n",
        "   - forward pass,\n",
        "   - backward pass,\n",
        "   - optimizer step.\n",
        "   Any deviation from this pattern risks using stale gradients or no gradients at all.\n",
        "\n",
        "4. When train loss continues to decrease but test loss stops decreasing and\n",
        "   starts to increase, this is a sign of overfitting. Possible adjustments\n",
        "   include:\n",
        "   - reducing the number of epochs or using early stopping,\n",
        "   - increasing regularization (for example, weight decay or data augmentation),\n",
        "   - reducing model capacity (smaller hidden or latent dimensions),\n",
        "   - or adjusting the learning rate schedule.\n",
        "   In VAEs, you also need to be careful that changes in regularization (for example,\n",
        "   changing beta) do not trigger posterior collapse.\n",
        "\n",
        "5. Posterior collapse can be diagnosed by looking specifically at the KL term:\n",
        "   - If `train_kl` and `test_kl` are both close to zero for many epochs,\n",
        "     while the reconstruction term keeps decreasing, it suggests that\n",
        "     the approximate posterior $q_\\phi(z | x)$ is staying very close to the prior\n",
        "     for most inputs, so `z` is not carrying much information about `x`.\n",
        "   - If you increase `beta` and see KL decrease further while reconstructions\n",
        "     do not qualitatively improve, this also points toward collapse.\n",
        "\n",
        "   Plotting `train_kl` and `test_kl` over epochs, together with the reconstruction\n",
        "   terms, helps you see whether the latent regularization is too strong or too weak,\n",
        "   and whether the model is actually using the latent variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f77ba77",
      "metadata": {},
      "source": [
        "## Section 5 - Qualitative analysis: reconstructions, samples, latent space\n",
        "\n",
        "Now that the VAE is trained, we want to *look* at what it has learned.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "1. Visualize **reconstructions** (input vs output).\n",
        "2. Generate **random samples** by decoding latent vectors $z \\sim \\mathcal{N}(0, I)$.\n",
        "3. For `LATENT_DIM = 2`, visualize the **latent space** and see how classes are arranged.\n",
        "4. Perform **latent interpolations** between two examples and inspect the decoded path.\n",
        "\n",
        "These qualitative views help us building intuition:\n",
        "\n",
        "- Reconstructions tell us whether the model is capturing the main structure of the data.\n",
        "- Prior samples tell us whether the latent space is well aligned with the prior.\n",
        "- Latent plots and interpolations show how the VAE organizes information in $z$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2b40c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_reconstructions(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    num_images: int = 8,\n",
        "    title: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot original images and their reconstructions in two rows.\n",
        "\n",
        "    - Top row:    original x\n",
        "    - Bottom row: reconstructed x_hat\n",
        "\n",
        "    num_images: number of images to show (per row).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x, _ = next(iter(data_loader))\n",
        "    x = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        recon_logits, mu, logvar = model(x)\n",
        "        # For visualization, convert logits to probabilities in [0, 1]\n",
        "        recon = None # TODO\n",
        "\n",
        "    # Clamp to [0, 1] just in case\n",
        "    # (this shouldnt be necessary if you implemented the previous line correctly)\n",
        "    recon = recon.clamp(0.0, 1.0)\n",
        "\n",
        "    B = x.size(0)\n",
        "    n = min(num_images, B)\n",
        "\n",
        "    # Reshape reconstructions back to image shape (B, C, H, W)\n",
        "    recon_images = recon.view(B, C, H, W)\n",
        "\n",
        "    # Select first n images\n",
        "    x_show = x[:n]\n",
        "    recon_show = recon_images[:n]\n",
        "\n",
        "    # Build a grid: first row originals, second row reconstructions\n",
        "    grid = vutils.make_grid(\n",
        "        torch.cat([x_show, recon_show], dim=0),\n",
        "        nrow=n,\n",
        "        padding=2,\n",
        "    )\n",
        "\n",
        "    npimg = grid.cpu().numpy()\n",
        "    plt.figure(figsize=(1.5 * n, 3))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title(\"Top: original, Bottom: reconstruction\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage after training:\n",
        "plot_reconstructions(model, test_loader, num_images=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c530f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_from_prior(\n",
        "    model: nn.Module,\n",
        "    num_samples: int = 64,\n",
        "    title: str = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Sample z ~ N(0, I) in latent space and decode to images.\n",
        "\n",
        "    This tests whether the learned decoder, when driven by the prior p(z),\n",
        "    produces images that look like the training data distribution.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_samples, model.latent_dim, device=device)\n",
        "        recon_logits = None # TODO\n",
        "        samples = torch.sigmoid(recon_logits)  # map logits to [0, 1]\n",
        "        samples = samples.view(num_samples, C, H, W)\n",
        "\n",
        "    # Build a grid of sampled images\n",
        "    grid = vutils.make_grid(samples, nrow=int(math.sqrt(num_samples)), padding=2)\n",
        "    npimg = grid.cpu().numpy()\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title(\"Samples from p(z) decoded by the VAE\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage after training:\n",
        "sample_from_prior(model, num_samples=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c152933",
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def animate_latent_space(\n",
        "    latent_snapshots: list,\n",
        "    interval: int = 800,\n",
        "):\n",
        "    \"\"\"\n",
        "    Animate how the 2D latent space (mu(x)) evolves over training.\n",
        "\n",
        "    latent_snapshots: list of (mu, labels) tuples, one per epoch:\n",
        "        - mu:     tensor of shape (N, 2)\n",
        "        - labels: tensor of shape (N,)\n",
        "\n",
        "    interval: delay between frames in milliseconds.\n",
        "\n",
        "    Returns:\n",
        "    - HTML animation object (to be displayed in Jupyter).\n",
        "    \"\"\"\n",
        "    if len(latent_snapshots) == 0:\n",
        "        print(\"No latent snapshots available. Did you pass latent_snapshots to train()?\")\n",
        "        return\n",
        "\n",
        "    # Use first snapshot to set up axes\n",
        "    mu0, labels0 = latent_snapshots[0]\n",
        "    mu0 = mu0.numpy()\n",
        "    labels0 = labels0.numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "    scatter = ax.scatter(\n",
        "        mu0[:, 0],\n",
        "        mu0[:, 1],\n",
        "        c=labels0,\n",
        "        s=5,\n",
        "        alpha=0.7,\n",
        "        cmap=\"tab10\",\n",
        "    )\n",
        "    ax.set_xlabel(r\"$z_1$\")\n",
        "    ax.set_ylabel(r\"$z_2$\")\n",
        "    ax.set_title(\"Latent space - epoch 1\")\n",
        "    ax.grid(alpha=0.2)\n",
        "    ax.set_aspect(\"equal\", \"box\")\n",
        "\n",
        "    # Fix limits across all epochs for a fair comparison\n",
        "    all_mu = torch.cat([snap[0] for snap in latent_snapshots], dim=0).numpy()\n",
        "    x_min, y_min = all_mu.min(axis=0) - 0.5\n",
        "    x_max, y_max = all_mu.max(axis=0) + 0.5\n",
        "    ax.set_xlim(x_min, x_max)\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "\n",
        "    # Colorbar once\n",
        "    cbar = plt.colorbar(scatter, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(\"class\")\n",
        "\n",
        "    def update(frame_idx):\n",
        "        mu, labels = latent_snapshots[frame_idx]\n",
        "        mu = mu.numpy()\n",
        "        labels = labels.numpy()\n",
        "\n",
        "        scatter.set_offsets(mu)\n",
        "        scatter.set_array(labels)\n",
        "        ax.set_title(f\"Latent space - epoch {frame_idx + 1}\")\n",
        "        return scatter,\n",
        "\n",
        "    anim = FuncAnimation(\n",
        "        fig,\n",
        "        update,\n",
        "        frames=len(latent_snapshots),\n",
        "        interval=interval,\n",
        "        blit=True,\n",
        "    )\n",
        "\n",
        "    plt.close(fig)  # avoid duplicate static plot\n",
        "\n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "# Only if LATENT_DIM == 2 and latent_snapshots were collected\n",
        "html_anim = animate_latent_space(latent_snapshots, interval=800)\n",
        "html_anim  # Jupyter will display the animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2df4ccf",
      "metadata": {},
      "outputs": [],
      "source": [
        "def interpolate_between_two(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    idx1: int = 0,\n",
        "    idx2: int = 1,\n",
        "    num_steps: int = 8,\n",
        "):\n",
        "    \"\"\"\n",
        "    Latent interpolation between two examples from a minibatch.\n",
        "\n",
        "    - Take two images x1, x2 from a batch (indices idx1, idx2).\n",
        "    - Encode them to mu(x1), mu(x2).\n",
        "    - Interpolate linearly between these two latent means:\n",
        "          z(t) = (1 - t) * mu(x1) + t * mu(x2),  t in [0, 1].\n",
        "    - Decode each z(t) to obtain images along the path.\n",
        "\n",
        "    The displayed grid contains:\n",
        "    [ original x1 ] [ decoded interpolations (including endpoints) ] [ original x2 ]\n",
        "\n",
        "    If the dataset defines `classes` (e.g. MNIST, FashionMNIST, CIFAR10 in torchvision),\n",
        "    the title will use the corresponding class names. Otherwise, it falls back to\n",
        "    numeric labels.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Take a single batch of data\n",
        "    x_batch, y_batch = next(iter(data_loader))\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    # Safety: clip indices to batch size\n",
        "    B = x_batch.size(0)\n",
        "    idx1_clipped = int(max(0, min(B - 1, idx1)))\n",
        "    idx2_clipped = int(max(0, min(B - 1, idx2)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mu, logvar = model.encoder(x_batch)\n",
        "\n",
        "        # Endpoint latent codes (posterior means)\n",
        "        z1 = mu[idx1_clipped : idx1_clipped + 1]  # (1, latent_dim)\n",
        "        z2 = mu[idx2_clipped : idx2_clipped + 1]  # (1, latent_dim)\n",
        "\n",
        "        # Interpolation coefficients t in [0, 1]\n",
        "        t_vals = torch.linspace(0.0, 1.0, steps=num_steps, device=device).view(-1, 1)\n",
        "\n",
        "        # Latent codes along the line segment\n",
        "        # z(t) = (1 - t) * z1 + t * z2\n",
        "        z_interp = (1.0 - t_vals) * z1 + t_vals * z2   # (num_steps, latent_dim)\n",
        "\n",
        "        # Decode the interpolated latents\n",
        "        recon_logits = model.decoder(z_interp)\n",
        "        recon = torch.sigmoid(recon_logits).clamp(0.0, 1.0)\n",
        "        recon_images = recon.view(num_steps, C, H, W)  # (num_steps, C, H, W)\n",
        "\n",
        "    # Original endpoint images (for visual reference)\n",
        "    x1_orig = x_batch[idx1_clipped].detach().cpu().unsqueeze(0)  # (1, C, H, W)\n",
        "    x2_orig = x_batch[idx2_clipped].detach().cpu().unsqueeze(0)  # (1, C, H, W)\n",
        "\n",
        "    # Concatenate: [x1_orig] + decoded path (including decodes of endpoints) + [x2_orig]\n",
        "    images_to_show = torch.cat([x1_orig, recon_images.cpu(), x2_orig], dim=0)\n",
        "\n",
        "    # Build a single-row grid\n",
        "    n_images = images_to_show.size(0)\n",
        "    grid = vutils.make_grid(images_to_show, nrow=n_images, padding=2)\n",
        "    npimg = grid.numpy()\n",
        "\n",
        "    # Build a title with label information if available\n",
        "    label1 = y_batch[idx1_clipped].item()\n",
        "    label2 = y_batch[idx2_clipped].item()\n",
        "\n",
        "    # Try to use human-readable class names if the dataset provides them\n",
        "    classes = getattr(data_loader.dataset, \"classes\", None)\n",
        "    if classes is not None and 0 <= label1 < len(classes) and 0 <= label2 < len(classes):\n",
        "        label1_str = f\"{label1} ({classes[label1]})\"\n",
        "        label2_str = f\"{label2} ({classes[label2]})\"\n",
        "    else:\n",
        "        label1_str = str(label1)\n",
        "        label2_str = str(label2)\n",
        "\n",
        "    title = f\"Latent interpolation: label {label1_str} → label {label2_str}\"\n",
        "\n",
        "    plt.figure(figsize=(1.5 * n_images, 2))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Example usage after training:\n",
        "interpolate_between_two(model, test_loader, idx1=0, idx2=1, num_steps=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9971480",
      "metadata": {},
      "source": [
        "### 5.1 Hierarchical VAEs and diffusion models\n",
        "\n",
        "The tools you just used (reconstructions, prior samples, latent plots, interpolations)\n",
        "are the basic *qualitative diagnostics* for generative models:\n",
        "\n",
        "- In **hierarchical VAEs**, you will have multiple latent layers\n",
        "  $z^{(L)}, \\dots, z^{(1)}$ instead of a single $z$.\n",
        "  You can:\n",
        "  - inspect reconstructions and samples as before,\n",
        "  - look at latent spaces at different levels (for example, $z^{(L)}$ as a high-level\n",
        "    representation and $z^{(1)}$ as a lower-level one),\n",
        "  - and study how information flows from coarse to fine latents.\n",
        "\n",
        "- In **diffusion models**, instead of a single latent $z$ you have a whole stochastic\n",
        "  trajectory that gradually transforms noise into data (or vice versa).\n",
        "  The same ideas appear in a different form:\n",
        "  - sampling from a simple noise distribution (Gaussian),\n",
        "  - transforming it through a learned process toward the data distribution,\n",
        "  - inspecting intermediate steps (analogous to \"latent interpolations\" in a time dimension).\n",
        "\n",
        "The mental picture to keep:\n",
        "- a **simple** distribution (standard Gaussian) in a **latent space**,\n",
        "- a **learned** transformation (decoder, hierarchy of decoders, or diffusion process),\n",
        "- a **complex** data distribution that emerges after applying that transformation.\n",
        "\n",
        "These concepts will reappear in the hierarchical VAE and diffusion notebooks;\n",
        "the diagnostics you used here will remain useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0040ecd",
      "metadata": {},
      "source": [
        "### Qualitative analysis - questions\n",
        "\n",
        "1. When you compare original images and reconstructions:\n",
        "   - What kinds of information does the VAE capture well?\n",
        "   - What kinds of details tend to be lost or blurred?\n",
        "\n",
        "2. When you sample from the prior $z \\sim \\mathcal{N}(0, I)$ and decode:\n",
        "   - Do the samples resemble the training data distribution?\n",
        "   - If you see many nonsense images, what might this say about the alignment\n",
        "     between $q_\\phi(z \\mid x)$ and the prior $p(z)$?\n",
        "\n",
        "3. In the 2D latent space plot (if `LATENT_DIM = 2`):\n",
        "   - Do different classes cluster in different regions?\n",
        "   - Are the clusters well separated or heavily overlapping?\n",
        "   - How does this relate to the KL regularization pressure?\n",
        "\n",
        "4. In the latent interpolation:\n",
        "   - Does the decoded path change smoothly from one example to the other?\n",
        "   - Do intermediate points look like plausible examples from the dataset,\n",
        "     or do they leave the data manifold and become unrealistic?\n",
        "\n",
        "5. Imagine you had a two-layer hierarchical VAE with latents $(z^{(2)}, z^{(1)})$.\n",
        "   How would you adapt the techniques from this section to inspect:\n",
        "   - the distribution and structure of $z^{(2)}$,\n",
        "   - the conditional structure of $z^{(1)}$ given $z^{(2)}$,\n",
        "   - and the final reconstructions and samples?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4314d7",
      "metadata": {},
      "source": [
        "### Qualitative analysis - answers\n",
        "\n",
        "1. VAEs with simple MLP architectures typically capture:\n",
        "   - global shape and coarse structure (digit identity, clothing type, rough pose),\n",
        "   - approximate intensity or stroke thickness.\n",
        "\n",
        "   They tend to lose:\n",
        "   - fine details (sharp edges, small texture),\n",
        "   - high-frequency noise patterns,\n",
        "   - small variations that require high-capacity decoders or more expressive likelihoods.\n",
        "\n",
        "   The use of a factorized Bernoulli or Gaussian likelihood and the bottleneck in $z$\n",
        "   both push the model toward \"average-looking\" reconstructions rather than very crisp ones.\n",
        "\n",
        "2. If prior samples decoded from $z \\sim \\mathcal{N}(0, I)$ resemble the training data,\n",
        "   this suggests that the encoder has organized $q_\\phi(z \\mid x)$ around the prior\n",
        "   and that the decoder has learned a good mapping from latent space to data space.\n",
        "   If many samples look like noise or nonsense images, possibilities include:\n",
        "   - the latent space learned by the encoder is concentrated in a small region of $\\mathbb{R}^{d_z}$,\n",
        "     and sampling from the full prior sends $z$ into regions the decoder has not seen during training,\n",
        "   - insufficient training or model capacity,\n",
        "   - imbalance between reconstruction and KL terms.\n",
        "\n",
        "3. In the 2D latent plot:\n",
        "   - clear clusters per class indicate that the encoder has learned to use $z$ to separate classes,\n",
        "     while the KL term still keeps the overall distribution roughly Gaussian-shaped,\n",
        "   - heavily overlapping clusters may reflect a strong KL term (posterior distributions pulled\n",
        "     too close to the prior) or insufficient model capacity.\n",
        "\n",
        "   The KL regularization encourages the overall cloud of points to look roughly like a standard\n",
        "   normal, while the reconstruction objective encourages class structure to emerge within that cloud.\n",
        "\n",
        "4. For a good VAE, latent interpolations:\n",
        "   - should produce a smooth series of images where high-level attributes (for example,\n",
        "     digit identity) change gradually,\n",
        "   - and each intermediate should look like a plausible example from the dataset.\n",
        "\n",
        "   If interpolations quickly produce unrealistic images, it may indicate that the straight line\n",
        "   in latent space leaves the \"data manifold\" region that the decoder was trained on, or that\n",
        "   the latent space is not globally well organized.\n",
        "\n",
        "5. In a two-layer hierarchical VAE with $(z^{(2)}, z^{(1)})$ you could:\n",
        "\n",
        "   - Plot the distribution of $z^{(2)}$ (for example, in 2D or via projections) to see whether\n",
        "     it captures global factors (for example, rough class or style).\n",
        "   - Condition on a fixed $z^{(2)}$ and look at the distribution of $z^{(1)}$ and the corresponding\n",
        "     reconstructions to see how lower-level latents refine the structure.\n",
        "   - Perform interpolations at different levels:\n",
        "     - interpolate in $z^{(2)}$ while keeping $z^{(1)}$ fixed,\n",
        "     - interpolate in $z^{(1)}$ while keeping $z^{(2)}$ fixed,\n",
        "     to see the difference between coarse and fine control.\n",
        "\n",
        "   These are direct generalizations of the tools used in this section and will be useful\n",
        "   for understanding multi-layer and diffusion-based generative models.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "834e096c",
      "metadata": {},
      "source": [
        "## Section 6 - Experiments and extensions\n",
        "\n",
        "In this section you will run short experiments on your own.\n",
        "\n",
        "The goal is **not** to tune a perfect model, but to build intuition for how\n",
        "design choices affect:\n",
        "\n",
        "- reconstructions,\n",
        "- samples from the prior,\n",
        "- latent space structure.\n",
        "\n",
        "For each experiment:\n",
        "\n",
        "1. Change a small set of hyperparameters (latent dimension, beta, dataset).\n",
        "2. Re-train for a few epochs (not necessarily fully converged).\n",
        "3. Use the tools from Section 5:\n",
        "   - reconstructions,\n",
        "   - prior samples,\n",
        "   - (for `LATENT_DIM = 2`) latent plots and interpolations.\n",
        "4. Write down your observations and hypotheses in a separate markdown cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2406da",
      "metadata": {},
      "source": [
        "### 6.1 Experiment A - Varying the latent dimension\n",
        "\n",
        "**Question:** How does the latent dimension `LATENT_DIM` affect what the VAE learns?\n",
        "\n",
        "Suggested settings (you can change them):\n",
        "\n",
        "- Dataset: keep `DATASET_NAME = \"MNIST\"` or `\"FashionMNIST\"`.\n",
        "- Hidden dimension: keep `HIDDEN_DIM` fixed (e.g. 512).\n",
        "- Latent dimensions to try:\n",
        "  - `LATENT_DIM = 2` (for visualization),\n",
        "  - `LATENT_DIM = 10`,\n",
        "  - `LATENT_DIM = 100`.\n",
        "\n",
        "For each choice:\n",
        "\n",
        "1. Set `LATENT_DIM` in the config cell.\n",
        "2. Re-instantiate the model and optimizer.\n",
        "3. Train for a small number of epochs (e.g. 5-10).\n",
        "4. Use:\n",
        "   - `plot_reconstructions(...)`,\n",
        "   - `sample_from_prior(...)`,\n",
        "   - and if `LATENT_DIM == 2`, `plot_latent_space(...)` and `interpolate_between_two(...)`.\n",
        "\n",
        "**Write down:**\n",
        "\n",
        "- How do reconstructions change as `LATENT_DIM` increases?\n",
        "- How do samples from the prior change?\n",
        "- For `LATENT_DIM = 2`, what does the latent space look like (clusters, overlaps)?\n",
        "- For `LATENT_DIM = 10` or `100`, even if you cannot visualize the full latent directly,\n",
        "  can you see any change in the *quality* or *diversity* of reconstructions and samples?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5572f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: run one configuration (you can repeat with different LATENT_DIM)\n",
        "LATENT_DIM = 2   # change to 10, 32, ...\n",
        "\n",
        "# Rebuild model and optimizer with new LATENT_DIM\n",
        "model = VAE(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Short training run (adjust NUM_EPOCHS if needed)\n",
        "latent_snapshots = []\n",
        "history = train(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    beta=1.0,\n",
        "    latent_snapshots=latent_snapshots,\n",
        "    latent_loader=test_loader, \n",
        ")\n",
        "\n",
        "# Qualitative analysis\n",
        "plot_reconstructions(model, test_loader, num_images=8, title=f\"Reconstructions (LATENT_DIM={LATENT_DIM})\")\n",
        "sample_from_prior(model, num_samples=64, title=f\"Samples (LATENT_DIM={LATENT_DIM})\")\n",
        "\n",
        "html_anim = None\n",
        "if LATENT_DIM == 2:\n",
        "    interpolate_between_two(model, test_loader, idx1=0, idx2=1, num_steps=10)\n",
        "    html_anim = animate_latent_space(latent_snapshots, interval=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a980ea3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "html_anim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e2843d3",
      "metadata": {},
      "source": [
        "### 6.2 Experiment B - Beta-VAE: varying β\n",
        "\n",
        "We now fix the architecture and dataset, and change only the KL weight β in:\n",
        "\n",
        "    loss = recon + β * KL(q(z | x) || p(z))\n",
        "\n",
        "**Question:** How does β trade off reconstruction quality and latent regularity?\n",
        "\n",
        "Suggested settings:\n",
        "\n",
        "- Keep `DATASET_NAME` and `LATENT_DIM` fixed (for example, `\"MNIST\"` and `LATENT_DIM = 2` or `10`).\n",
        "- Try:\n",
        "  - `beta = 0.1` (weak regularization),\n",
        "  - `beta = 1.0` (standard VAE),\n",
        "  - `beta = 4.0` (strong regularization).\n",
        "\n",
        "For each β:\n",
        "\n",
        "1. Re-instantiate the model and optimizer (do not reuse weights from previous runs).\n",
        "2. Train for the same number of epochs.\n",
        "3. Plot training curves with `plot_training_curves(history)` if you stored `history`.\n",
        "4. Inspect reconstructions, prior samples, and (if `LATENT_DIM = 2`) the latent space.\n",
        "\n",
        "**Write down:**\n",
        "\n",
        "- How do the **reconstruction** and **KL** curves change with β?  \n",
        "  Does KL go up or down as β increases?\n",
        "- Visually, how do reconstructions and samples differ across β?  \n",
        "  Which β gives “sharper” reconstructions? Which β gives a more “Gaussian-looking” latent space?\n",
        "- Do you see any sign of **posterior collapse** for large β (for example, very small KL and poor use of z)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c023f4f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: run one configuration for a given beta\n",
        "LATENT_DIM = 2\n",
        "BETA = 4.0   # change to 0.1, 1.0, 4.0, ...\n",
        "\n",
        "model = VAE(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "latent_snapshots = []\n",
        "history = train(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    beta=BETA,\n",
        "    latent_snapshots=latent_snapshots,\n",
        "    latent_loader=test_loader, \n",
        ")\n",
        "\n",
        "plot_training_curves(history)\n",
        "\n",
        "plot_reconstructions(model, test_loader, num_images=8, title=f\"Reconstructions (beta={BETA})\")\n",
        "sample_from_prior(model, num_samples=64, title=f\"Samples (beta={BETA})\")\n",
        "\n",
        "html_anim = None\n",
        "if LATENT_DIM == 2:\n",
        "    html_anim = animate_latent_space(latent_snapshots, interval=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f6c8db",
      "metadata": {},
      "outputs": [],
      "source": [
        "html_anim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297098fc",
      "metadata": {},
      "source": [
        "### 6.3 Experiment C - Changing the dataset\n",
        "\n",
        "So far you may have used a single dataset as a default. Here you will change the dataset\n",
        "and observe how the *same* architecture behaves.\n",
        "\n",
        "Suggested experiments:\n",
        "\n",
        "1. Simple switch:\n",
        "   - Change `DATASET_NAME` to `\"MNIST\"` / `\"FashionMNIST\"` / `\"KMNIST\"`.\n",
        "   - Keep the architecture and training hyperparameters unchanged.\n",
        "2. More challenging switch:\n",
        "   - Set `DATASET_NAME = \"CIFAR10\"`.\n",
        "   - Keep the MLP architecture (fully connected encoder/decoder).\n",
        "   - Observe what happens.\n",
        "\n",
        "For each dataset:\n",
        "\n",
        "1. Go back to the configuration cell and change `DATASET_NAME`.\n",
        "2. Re-run:\n",
        "   - dataset loading and shape inspection,\n",
        "   - model and optimizer instantiation,\n",
        "   - training loop for a few epochs,\n",
        "   - qualitative analysis (reconstructions, prior samples, latent visualization if applicable).\n",
        "\n",
        "**Write down:**\n",
        "\n",
        "- How do reconstructions differ across datasets (MNIST vs FashionMNIST vs KMNIST vs CIFAR10)?\n",
        "- For CIFAR10 with this simple MLP:\n",
        "  - How good are reconstructions?\n",
        "  - How realistic are samples from the prior?\n",
        "  - What does this tell you about the limitations of MLP-based VAEs for natural images?\n",
        "- Based on your observations, what architectural changes would you propose\n",
        "  (for example, convolutions, deeper networks, different likelihoods)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98d25a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: run on a new dataset\n",
        "DATASET_NAME = \"FashionMNIST\"  # or \"KMNIST\", \"CIFAR10\"\n",
        "print(f\"Using dataset: {DATASET_NAME}\")\n",
        "\n",
        "# Reload datasets\n",
        "train_dataset, test_dataset = get_datasets(DATASET_NAME)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Inspect shapes once\n",
        "example_batch, example_labels = next(iter(train_loader))\n",
        "print(\"New batch shape:\", example_batch.shape)\n",
        "\n",
        "C, H, W = example_batch.shape[1:]\n",
        "INPUT_DIM = C * H * W\n",
        "\n",
        "# Rebuild model and optimizer\n",
        "LATENT_DIM = 2   # or higher if you prefer\n",
        "model = VAE(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training\n",
        "latent_snapshots = []\n",
        "history = train(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    beta=1.0,\n",
        "    latent_snapshots=latent_snapshots,\n",
        "    latent_loader=test_loader, \n",
        ")\n",
        "\n",
        "# Qualitative analysis\n",
        "plot_reconstructions(model, test_loader, num_images=8, title=f\"Reconstructions ({DATASET_NAME})\")\n",
        "sample_from_prior(model, num_samples=64, title=f\"Samples ({DATASET_NAME})\")\n",
        "\n",
        "html_anim = None\n",
        "if LATENT_DIM == 2:\n",
        "    html_anim = animate_latent_space(latent_snapshots, interval=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8dfb3af",
      "metadata": {},
      "outputs": [],
      "source": [
        "html_anim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c81853",
      "metadata": {},
      "source": [
        "### 6.4 Optional - Design your own modification\n",
        "\n",
        "Pick one of the following (or propose your own):\n",
        "\n",
        "- Replace the MLP encoder/decoder with simple convolutional architectures\n",
        "  for MNIST or CIFAR10.\n",
        "- Change the reconstruction loss from Bernoulli (BCE with logits) to MSE,\n",
        "  and compare reconstructions and samples.\n",
        "- Add a very simple learning rate schedule and see if convergence improves.\n",
        "- Try a different optimizer (for example, SGD with momentum) and compare behavior.\n",
        "\n",
        "For your chosen modification:\n",
        "\n",
        "1. Implement the change.\n",
        "2. Re-train on one dataset for a few epochs.\n",
        "3. Compare to the original baseline using:\n",
        "   - training curves,\n",
        "   - reconstructions,\n",
        "   - samples,\n",
        "   - and (when applicable) the latent space.\n",
        "\n",
        "Summarize your findings in a short paragraph: what changed, and why do you\n",
        "think this is happening?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab553790",
      "metadata": {},
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c079dac2",
      "metadata": {},
      "source": [
        "# Part 2 - Two-level (hierarchical) VAE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d11637a",
      "metadata": {},
      "source": [
        "In Part 1 you implemented a *single latent* VAE:\n",
        "\n",
        "- one latent variable $z$ with prior $p(z) = \\mathcal{N}(0, I)$,\n",
        "- encoder $q_\\phi(z \\mid x)$ and decoder $p_\\theta(x \\mid z)$,\n",
        "- ELBO with one reconstruction term and one KL term.\n",
        "\n",
        "In this part we extend this to a **two-level hierarchical VAE** with latents\n",
        "\n",
        "- $z^{(2)}$ (top, coarse),\n",
        "- $z^{(1)}$ (bottom, closer to pixels).\n",
        "\n",
        "The idea:\n",
        "\n",
        "- $z^{(2)}$ captures more global information (e.g. class, style),\n",
        "- $z^{(1)}$ captures more local details,\n",
        "- the prior of $z^{(1)}$ is *conditioned* on $z^{(2)}$.\n",
        "\n",
        "We keep the architecture small and MLP-based so we can focus on the **probabilistic structure**\n",
        "and the ELBO, not on engineering.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6126e84",
      "metadata": {},
      "source": [
        "## 2.1 Generative and inference story\n",
        "\n",
        "We introduce two latent layers:\n",
        "\n",
        "- Top latent: $z^{(2)} \\in \\mathbb{R}^{d_2}$.\n",
        "- Bottom latent: $z^{(1)} \\in \\mathbb{R}^{d_1}$.\n",
        "\n",
        "**Generative model (top-down)**\n",
        "\n",
        "1. Sample top latent\n",
        "   $$\n",
        "   z^{(2)} \\sim p(z^{(2)}) = \\mathcal{N}(0, I).\n",
        "   $$\n",
        "2. Conditional prior for bottom latent\n",
        "   $$\n",
        "   p_\\theta(z^{(1)} \\mid z^{(2)}) =\n",
        "   \\mathcal{N}\\big( \\mu_\\theta^{(1)}(z^{(2)}),\n",
        "                    \\mathrm{diag}(\\sigma_\\theta^{(1)}(z^{(2)})^2) \\big).\n",
        "   $$\n",
        "3. Likelihood of data\n",
        "   $$\n",
        "   p_\\theta(x \\mid z^{(1)}) \\quad \\text{(Bernoulli with logits from a decoder)}.\n",
        "   $$\n",
        "\n",
        "So the generative story is:\n",
        "\n",
        "- sample $z^{(2)}$,\n",
        "- sample $z^{(1)}$ given $z^{(2)}$,\n",
        "- sample $x$ given $z^{(1)}$.\n",
        "\n",
        "**Inference model (bottom-up, factorized approximation)**\n",
        "\n",
        "For simplicity, we use:\n",
        "\n",
        "- $q_\\phi(z^{(1)} \\mid x)$ – bottom encoder,\n",
        "- $q_\\phi(z^{(2)} \\mid x)$ – top encoder,\n",
        "\n",
        "and approximate\n",
        "\n",
        "$$\n",
        "q_\\phi(z^{(1)}, z^{(2)} \\mid x)\n",
        "\\approx q_\\phi(z^{(1)} \\mid x)\\,q_\\phi(z^{(2)} \\mid x).\n",
        "$$\n",
        "\n",
        "**ELBO (one-sample approximation)**\n",
        "\n",
        "For a single $x$, the ELBO can be written as\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(x) \\approx\n",
        "\\mathbb{E}_{q_\\phi(z^{(1)} \\mid x)}[\\log p_\\theta(x \\mid z^{(1)})]\n",
        "- \\mathbb{E}_{q_\\phi(z^{(2)} \\mid x)}\\big[ \\mathrm{KL}\\big(q_\\phi(z^{(1)} \\mid x)\n",
        "    \\Vert p_\\theta(z^{(1)} \\mid z^{(2)})\\big) \\big]\n",
        "- \\mathrm{KL}\\big(q_\\phi(z^{(2)} \\mid x) \\Vert p(z^{(2)})\\big).\n",
        "$$\n",
        "\n",
        "In code we will:\n",
        "\n",
        "- sample one $z^{(2)} \\sim q_\\phi(z^{(2)} \\mid x)$,\n",
        "- use that sample to define $p_\\theta(z^{(1)} \\mid z^{(2)})$,\n",
        "- compute a KL between **two diagonal Gaussians** for $z^{(1)}$.\n",
        "\n",
        "You can view this as:\n",
        "\n",
        "- reconstruction term for $x$ given $z^{(1)}$,\n",
        "- KL for the *top latent* versus a standard normal,\n",
        "- KL for the *bottom latent* versus a **data-dependent Gaussian prior**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af388fc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def kl_standard_normal(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    KL(q(z) || p(z)) where:\n",
        "        q(z) = N(mu, diag(exp(logvar)))\n",
        "        p(z) = N(0, I)\n",
        "\n",
        "    Closed form per dimension:\n",
        "        KL = -0.5 * (1 + logvar - mu^2 - exp(logvar))\n",
        "\n",
        "    Returns:\n",
        "        scalar KL averaged over batch.\n",
        "    \"\"\"\n",
        "    kl_per_dim = -0.5 * (1.0 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kl = kl_per_dim.sum(dim=1).mean()\n",
        "    return kl\n",
        "\n",
        "\n",
        "def kl_diag_gaussians(\n",
        "    mu_q: torch.Tensor,\n",
        "    logvar_q: torch.Tensor,\n",
        "    mu_p: torch.Tensor,\n",
        "    logvar_p: torch.Tensor,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    KL( N(mu_q, diag(sigma_q^2)) || N(mu_p, diag(sigma_p^2)) )\n",
        "\n",
        "    For each dimension i:\n",
        "        KL_i = 0.5 * [ log(sigma_p^2 / sigma_q^2)\n",
        "                       + (sigma_q^2 + (mu_q - mu_p)^2) / sigma_p^2\n",
        "                       - 1 ]\n",
        "\n",
        "    Args:\n",
        "        mu_q, logvar_q: [batch, d] parameters of q\n",
        "        mu_p, logvar_p: [batch, d] parameters of p\n",
        "\n",
        "    Returns:\n",
        "        scalar KL averaged over batch.\n",
        "    \"\"\"\n",
        "    sigma2_q = logvar_q.exp()\n",
        "    sigma2_p = logvar_p.exp()\n",
        "\n",
        "    # log(sigma_p^2 / sigma_q^2)\n",
        "    log_ratio = logvar_p - logvar_q\n",
        "\n",
        "    # (sigma_q^2 + (mu_q - mu_p)^2) / sigma_p^2\n",
        "    sq_diff = (mu_q - mu_p).pow(2)\n",
        "    frac = (sigma2_q + sq_diff) / sigma2_p\n",
        "\n",
        "    kl_per_dim = 0.5 * (log_ratio + frac - 1.0)\n",
        "    kl = kl_per_dim.sum(dim=1).mean()\n",
        "    return kl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99bae069",
      "metadata": {},
      "outputs": [],
      "source": [
        "class HierarchicalVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        latent_dim_bottom: int,\n",
        "        latent_dim_top: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim_bottom = latent_dim_bottom\n",
        "        self.latent_dim_top = latent_dim_top\n",
        "\n",
        "        # Encoder for bottom latent z1: q(z1 | x)\n",
        "        self.enc1_fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.enc1_mu = nn.Linear(hidden_dim, latent_dim_bottom)\n",
        "        self.enc1_logvar = nn.Linear(hidden_dim, latent_dim_bottom)\n",
        "\n",
        "        # Encoder for top latent z2: q(z2 | x)\n",
        "        self.enc2_fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.enc2_mu = nn.Linear(hidden_dim, latent_dim_top)\n",
        "        self.enc2_logvar = nn.Linear(hidden_dim, latent_dim_top)\n",
        "\n",
        "        # Conditional prior for z1 given z2: p(z1 | z2)\n",
        "        self.prior1_fc1 = nn.Linear(latent_dim_top, hidden_dim)\n",
        "        self.prior1_mu = nn.Linear(hidden_dim, latent_dim_bottom)\n",
        "        self.prior1_logvar = nn.Linear(hidden_dim, latent_dim_bottom)\n",
        "\n",
        "        # Decoder from bottom latent z1 to x: p(x | z1)\n",
        "        self.dec_fc1 = nn.Linear(latent_dim_bottom, hidden_dim)\n",
        "        self.dec_out = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def encode_bottom(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        q(z1 | x)\n",
        "        \"\"\"\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        h = self.activation(self.enc1_fc1(x))\n",
        "        mu1 = self.enc1_mu(h)\n",
        "        logvar1 = self.enc1_logvar(h)\n",
        "        return mu1, logvar1\n",
        "\n",
        "    def encode_top(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        q(z2 | x)\n",
        "        \"\"\"\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        h = self.activation(self.enc2_fc1(x))\n",
        "        mu2 = self.enc2_mu(h)\n",
        "        logvar2 = self.enc2_logvar(h)\n",
        "        return mu2, logvar2\n",
        "\n",
        "    def prior_bottom(self, z2: torch.Tensor):\n",
        "        \"\"\"\n",
        "        p(z1 | z2) as a diagonal Gaussian.\n",
        "        \"\"\"\n",
        "        h = self.activation(self.prior1_fc1(z2))\n",
        "        mu1_p = self.prior1_mu(h)\n",
        "        logvar1_p = self.prior1_logvar(h)\n",
        "        return mu1_p, logvar1_p\n",
        "\n",
        "    def decode(self, z1: torch.Tensor):\n",
        "        \"\"\"\n",
        "        p(x | z1) logits (flattened images).\n",
        "        \"\"\"\n",
        "        h = self.activation(self.dec_fc1(z1))\n",
        "        logits = self.dec_out(h)\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Full hierarchical VAE forward:\n",
        "\n",
        "        1) Encode top latent q(z2 | x), sample z2.\n",
        "        2) Encode bottom latent q(z1 | x), sample z1.\n",
        "        3) Compute conditional prior p(z1 | z2).\n",
        "        4) Decode x_hat from z1.\n",
        "\n",
        "        Returns:\n",
        "            recon_logits, mu1_q, logvar1_q, mu2_q, logvar2_q, mu1_p, logvar1_p\n",
        "        \"\"\"\n",
        "        # q(z2 | x)\n",
        "        mu2_q, logvar2_q = self.encode_top(x)\n",
        "        z2 = reparameterize(mu2_q, logvar2_q)\n",
        "\n",
        "        # q(z1 | x)\n",
        "        mu1_q, logvar1_q = self.encode_bottom(x)\n",
        "        z1 = reparameterize(mu1_q, logvar1_q)\n",
        "\n",
        "        # p(z1 | z2)\n",
        "        mu1_p, logvar1_p = self.prior_bottom(z2)\n",
        "\n",
        "        # p(x | z1)\n",
        "        recon_logits = self.decode(z1)\n",
        "\n",
        "        return recon_logits, mu1_q, logvar1_q, mu2_q, logvar2_q, mu1_p, logvar1_p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78583100",
      "metadata": {},
      "outputs": [],
      "source": [
        "def hierarchical_vae_loss(\n",
        "    recon_logits: torch.Tensor,\n",
        "    x: torch.Tensor,\n",
        "    mu1_q: torch.Tensor,\n",
        "    logvar1_q: torch.Tensor,\n",
        "    mu2_q: torch.Tensor,\n",
        "    logvar2_q: torch.Tensor,\n",
        "    mu1_p: torch.Tensor,\n",
        "    logvar1_p: torch.Tensor,\n",
        "    beta_top: float = 1.0,\n",
        "    beta_bottom: float = 1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the hierarchical VAE loss:\n",
        "\n",
        "        recon = reconstruction loss for p(x | z1)\n",
        "        kl_top = KL(q(z2 | x) || p(z2)) with p(z2) = N(0, I)\n",
        "        kl_bottom = KL(q(z1 | x) || p(z1 | z2))\n",
        "\n",
        "    Total loss:\n",
        "        loss = recon + beta_top * kl_top + beta_bottom * kl_bottom\n",
        "    \"\"\"\n",
        "    recon = reconstruction_loss(recon_logits, x)\n",
        "    kl_top = kl_standard_normal(mu2_q, logvar2_q)\n",
        "    kl_bottom = kl_diag_gaussians(mu1_q, logvar1_q, mu1_p, logvar1_p)\n",
        "    loss = recon + beta_top * kl_top + beta_bottom * kl_bottom\n",
        "    return loss, recon, kl_top, kl_bottom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0cd1a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration specific to the hierarchical VAE\n",
        "HIDDEN_DIM_HVAE = 512\n",
        "LATENT_DIM_BOTTOM = 16   # z1\n",
        "LATENT_DIM_TOP = 2       # z2 (2D for visualization)\n",
        "\n",
        "hvae = HierarchicalVAE(\n",
        "    input_dim=INPUT_DIM,\n",
        "    hidden_dim=HIDDEN_DIM_HVAE,\n",
        "    latent_dim_bottom=LATENT_DIM_BOTTOM,\n",
        "    latent_dim_top=LATENT_DIM_TOP,\n",
        ").to(device)\n",
        "\n",
        "hvae_optimizer = torch.optim.Adam(hvae.parameters(), lr=1e-3)\n",
        "\n",
        "print(hvae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb77832",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_one_epoch_hvae(\n",
        "    epoch: int,\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    beta_top: float = 1.0,\n",
        "    beta_bottom: float = 1.0,\n",
        "):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_recon = 0.0\n",
        "    running_kl_top = 0.0\n",
        "    running_kl_bottom = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (x, _) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        (\n",
        "            recon_logits,\n",
        "            mu1_q, logvar1_q,\n",
        "            mu2_q, logvar2_q,\n",
        "            mu1_p, logvar1_p,\n",
        "        ) = model(x)\n",
        "\n",
        "        loss, recon, kl_top, kl_bottom = hierarchical_vae_loss(\n",
        "            recon_logits,\n",
        "            x,\n",
        "            mu1_q, logvar1_q,\n",
        "            mu2_q, logvar2_q,\n",
        "            mu1_p, logvar1_p,\n",
        "            beta_top=beta_top,\n",
        "            beta_bottom=beta_bottom,\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_recon += recon.item()\n",
        "        running_kl_top += kl_top.item()\n",
        "        running_kl_bottom += kl_bottom.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = running_loss / num_batches\n",
        "            avg_recon = running_recon / num_batches\n",
        "            avg_kl_top = running_kl_top / num_batches\n",
        "            avg_kl_bottom = running_kl_bottom / num_batches\n",
        "            print(\n",
        "                f\"[HVAE] Epoch {epoch:03d} \"\n",
        "                f\"Batch {batch_idx + 1:04d}/{len(dataloader):04d} \"\n",
        "                f\"Loss: {avg_loss:.2f}  \"\n",
        "                f\"Recon: {avg_recon:.2f}  \"\n",
        "                f\"KL_top: {avg_kl_top:.2f}  \"\n",
        "                f\"KL_bottom: {avg_kl_bottom:.2f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = running_loss / num_batches\n",
        "    avg_recon = running_recon / num_batches\n",
        "    avg_kl_top = running_kl_top / num_batches\n",
        "    avg_kl_bottom = running_kl_bottom / num_batches\n",
        "\n",
        "    return avg_loss, avg_recon, avg_kl_top, avg_kl_bottom\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_hvae(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    beta_top: float = 1.0,\n",
        "    beta_bottom: float = 1.0,\n",
        "):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_recon = 0.0\n",
        "    running_kl_top = 0.0\n",
        "    running_kl_bottom = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for x, _ in dataloader:\n",
        "        x = x.to(device)\n",
        "\n",
        "        (\n",
        "            recon_logits,\n",
        "            mu1_q, logvar1_q,\n",
        "            mu2_q, logvar2_q,\n",
        "            mu1_p, logvar1_p,\n",
        "        ) = model(x)\n",
        "\n",
        "        loss, recon, kl_top, kl_bottom = hierarchical_vae_loss(\n",
        "            recon_logits,\n",
        "            x,\n",
        "            mu1_q, logvar1_q,\n",
        "            mu2_q, logvar2_q,\n",
        "            mu1_p, logvar1_p,\n",
        "            beta_top=beta_top,\n",
        "            beta_bottom=beta_bottom,\n",
        "        )\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_recon += recon.item()\n",
        "        running_kl_top += kl_top.item()\n",
        "        running_kl_bottom += kl_bottom.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = running_loss / num_batches\n",
        "    avg_recon = running_recon / num_batches\n",
        "    avg_kl_top = running_kl_top / num_batches\n",
        "    avg_kl_bottom = running_kl_bottom / num_batches\n",
        "\n",
        "    return avg_loss, avg_recon, avg_kl_top, avg_kl_bottom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904f19d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS_HVAE = 5\n",
        "beta_top = 1.0\n",
        "beta_bottom = 1.0\n",
        "\n",
        "hvae_history = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_recon\": [],\n",
        "    \"train_kl_top\": [],\n",
        "    \"train_kl_bottom\": [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_recon\": [],\n",
        "    \"test_kl_top\": [],\n",
        "    \"test_kl_bottom\": [],\n",
        "}\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS_HVAE + 1):\n",
        "    train_loss, train_recon, train_kl_top, train_kl_bottom = train_one_epoch_hvae(\n",
        "        epoch,\n",
        "        hvae,\n",
        "        train_loader,\n",
        "        hvae_optimizer,\n",
        "        beta_top=beta_top,\n",
        "        beta_bottom=beta_bottom,\n",
        "    )\n",
        "    test_loss, test_recon, test_kl_top, test_kl_bottom = evaluate_hvae(\n",
        "        hvae,\n",
        "        test_loader,\n",
        "        beta_top=beta_top,\n",
        "        beta_bottom=beta_bottom,\n",
        "    )\n",
        "\n",
        "    hvae_history[\"train_loss\"].append(train_loss)\n",
        "    hvae_history[\"train_recon\"].append(train_recon)\n",
        "    hvae_history[\"train_kl_top\"].append(train_kl_top)\n",
        "    hvae_history[\"train_kl_bottom\"].append(train_kl_bottom)\n",
        "\n",
        "    hvae_history[\"test_loss\"].append(test_loss)\n",
        "    hvae_history[\"test_recon\"].append(test_recon)\n",
        "    hvae_history[\"test_kl_top\"].append(test_kl_top)\n",
        "    hvae_history[\"test_kl_bottom\"].append(test_kl_bottom)\n",
        "\n",
        "    print(\n",
        "        f\"[HVAE] Epoch {epoch:03d} \"\n",
        "        f\"| train loss {train_loss:.2f} (recon {train_recon:.2f}, KL_top {train_kl_top:.2f}, KL_bottom {train_kl_bottom:.2f}) \"\n",
        "        f\"| test loss {test_loss:.2f} (recon {test_recon:.2f}, KL_top {test_kl_top:.2f}, KL_bottom {test_kl_bottom:.2f})\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aa91888",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils as vutils\n",
        "\n",
        "def plot_reconstructions(\n",
        "    model: nn.Module,\n",
        "    data_loader: DataLoader,\n",
        "    num_images: int = 8,\n",
        "    title: str | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot original images and their reconstructions.\n",
        "\n",
        "    Works with:\n",
        "    - plain VAE: model(x) -> (recon_logits, mu, logvar)\n",
        "    - hierarchical VAE: model(x) -> (recon_logits, mu1_q, logvar1_q, mu2_q, logvar2_q, mu1_p, logvar1_p)\n",
        "\n",
        "    Only the first output (recon_logits) is used here.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    x, _ = next(iter(data_loader))\n",
        "    x = x.to(device)\n",
        "    x = x[:num_images]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        # Accept either a single tensor or a tuple/list with recon_logits first\n",
        "        if isinstance(out, (tuple, list)):\n",
        "            recon_logits = out[0]\n",
        "        else:\n",
        "            recon_logits = out\n",
        "\n",
        "        recon = torch.sigmoid(recon_logits).clamp(0.0, 1.0)\n",
        "        recon = recon.view(-1, C, H, W)[:num_images]\n",
        "\n",
        "    # Build grid with originals on top and reconstructions below\n",
        "    originals = x.view(-1, C, H, W).cpu()\n",
        "    recon_cpu = recon.cpu()\n",
        "    stacked = torch.cat([originals, recon_cpu], dim=0)\n",
        "\n",
        "    grid = vutils.make_grid(stacked, nrow=num_images, padding=2)\n",
        "    npimg = grid.numpy()\n",
        "\n",
        "    plt.figure(figsize=(1.5 * num_images, 3))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    if title is None:\n",
        "        title = \"Originals (top) and reconstructions (bottom)\"\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9659662",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_reconstructions(\n",
        "    model=hvae,\n",
        "    data_loader=test_loader,\n",
        "    num_images=8,\n",
        "    title=\"HVAE reconstructions (top+bottom latents)\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa6b5485",
      "metadata": {},
      "source": [
        "## 2.2 Inspecting the top latent space\n",
        "\n",
        "Here we look only at the **top latent** $z^{(2)}$ (2D) and color the points by class.\n",
        "\n",
        "Compare this to:\n",
        "\n",
        "- the single latent 2D VAE from Part 1,\n",
        "- or a 2D latent in a non-hierarchical VAE.\n",
        "\n",
        "Questions to think about:\n",
        "\n",
        "- Do the classes cluster more clearly in $z^{(2)}$?\n",
        "- Does $z^{(2)}$ capture higher-level structure than $z^{(1)}$?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9d5b45",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def plot_top_latent_space(\n",
        "    model: HierarchicalVAE,\n",
        "    data_loader: DataLoader,\n",
        "    max_batches: int = 100,\n",
        "):\n",
        "    if model.latent_dim_top != 2:\n",
        "        print(\"Top latent dim is not 2; cannot make a 2D scatter.\")\n",
        "        return\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_mu2 = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(data_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        mu2_q, logvar2_q = model.encode_top(x)\n",
        "        all_mu2.append(mu2_q)\n",
        "        all_labels.append(y)\n",
        "\n",
        "        if batch_idx + 1 >= max_batches:\n",
        "            break\n",
        "\n",
        "    mu2_all = torch.cat(all_mu2, dim=0).cpu()\n",
        "    labels_all = torch.cat(all_labels, dim=0).cpu()\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    scatter = plt.scatter(\n",
        "        mu2_all[:, 0],\n",
        "        mu2_all[:, 1],\n",
        "        c=labels_all.numpy(),\n",
        "        s=5,\n",
        "        alpha=0.7,\n",
        "        cmap=\"tab10\",\n",
        "    )\n",
        "    plt.xlabel(r\"$z^{(2)}_1$\")\n",
        "    plt.ylabel(r\"$z^{(2)}_2$\")\n",
        "    plt.title(\"Top latent space z2 (HVAE) colored by class\")\n",
        "    plt.grid(alpha=0.2)\n",
        "    plt.gca().set_aspect(\"equal\", \"box\")\n",
        "    plt.colorbar(scatter, fraction=0.046, pad=0.04)\n",
        "    plt.show()\n",
        "\n",
        "# Example:\n",
        "plot_top_latent_space(hvae, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed708123",
      "metadata": {},
      "source": [
        "## 2.3 Reflection and connection to deeper models\n",
        "\n",
        "You have now implemented:\n",
        "\n",
        "- a **single latent** VAE (Part 1),\n",
        "- a **two-level hierarchical** VAE (Part 2).\n",
        "\n",
        "In more advanced models such as **NVAE**:\n",
        "\n",
        "- the architecture is convolutional and deep,\n",
        "- there are *many* latent levels (e.g. per resolution scale),\n",
        "- priors can be more expressive (e.g. using normalizing flows or autoregressive components),\n",
        "- but the core ideas you used are still there:\n",
        "  - latent hierarchy $z^{(L)}, \\dots, z^{(1)}$,\n",
        "  - conditional priors $p(z^{(\\ell)} \\mid z^{(\\ell+1)})$,\n",
        "  - ELBO with one reconstruction term and multiple KL terms.\n",
        "\n",
        "When you move to **diffusion models**, you again see:\n",
        "\n",
        "- a simple noise distribution (Gaussian),\n",
        "- a learned transformation that maps noise to data,\n",
        "- objectives related to likelihood and uncertainty.\n",
        "\n",
        "**Questions to think about:**\n",
        "\n",
        "1. Compare the 2D latent space of:\n",
        "   - (a) the single latent VAE from Part 1 with `LATENT_DIM = 2`, and\n",
        "   - (b) the top latent $z^{(2)}$ in this hierarchical VAE.\n",
        "   Which one seems more “semantic” or class-clustered?\n",
        "\n",
        "2. In your hierarchical VAE runs, do you see:\n",
        "   - non-zero KL at the top level (z2),\n",
        "   - non-zero KL at the bottom level (z1)?\n",
        "   If one KL is almost zero, what does that say about how information is being used?\n",
        "\n",
        "3. If you had a *third* latent level $z^{(3)}$, where would you expect the most\n",
        "   abstract information to sit? What kinds of diagnostics from this notebook\n",
        "   would you reuse to verify that?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ede9ad",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
