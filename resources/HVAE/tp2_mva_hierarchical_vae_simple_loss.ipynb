{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2477ce57",
      "metadata": {
        "id": "2477ce57"
      },
      "source": [
        "# Hierarchical VAE\n",
        "\n",
        "\n",
        "This practice session allows to train a top-down hierarchical VAE.\n",
        "\n",
        "This is a toy implementation of the NVAE model with a simple MSE reconstruction loss and only three stages.\n",
        "\n",
        "![alternatvie text](https://raw.githubusercontent.com/generativemodelingmva/generativemodelingmva.github.io/main/tp2324/toynvae_framework_full.png)\n",
        "\n",
        "\n",
        "Sources:\n",
        "* Toy implementation from: https://github.com/GlassyWing/nvae\n",
        "* NVAE official implementation: https://github.com/NVlabs/NVAE\n",
        "* NVAE paper: \"NVAE: A Deep Hierarchical Variational Autoencoder\", Arash Vahdat and Jan Kautz (NeurIPS 2020 Spotlight Paper) https://arxiv.org/abs/2007.03898\n",
        "* CelebA validation set (used for training): https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f74528",
      "metadata": {
        "id": "05f74528"
      },
      "source": [
        "# Download files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "58483fde",
      "metadata": {
        "id": "58483fde",
        "outputId": "a4279b75-c335-4d72-d7cc-6ec0d5c715d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-08 09:37:12--  https://www.dropbox.com/scl/fi/3d2le2wlu61nzkbxymfm3/celeba64png_val.zip?rlkey=ckesud01kwb8tualsd3s8zg5d\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com/cd/0/inline/C2oYsEDSsnnYTAt9dbQE_a7wzGg9DE__2-pBK47iO_QbGTsbJbeeOr4CUrmNm6DDAeOuQSW3YhV8-P8Y8-7sO2czvqUaO-tjEh-jYzAN5_9LjeV7yPHGnWl_EjRoXAYQjKA/file# [following]\n",
            "--2025-12-08 09:37:13--  https://uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com/cd/0/inline/C2oYsEDSsnnYTAt9dbQE_a7wzGg9DE__2-pBK47iO_QbGTsbJbeeOr4CUrmNm6DDAeOuQSW3YhV8-P8Y8-7sO2czvqUaO-tjEh-jYzAN5_9LjeV7yPHGnWl_EjRoXAYQjKA/file\n",
            "Resolving uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com (uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com (uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/C2qLcOGWYzeNHxAZ__Uqu830ppDxiX5UEth-lwH6C3n6bZb-7R9aQTg9GmAc0SD8buBiWEjRsiFZwBvNsZBX9uaw7GUD2ZR9dJHJ5MpaPOGZxR8u_CQhEjsQVTTWd1OgVPZh0FBv25HnKjY96sa7ljYK-Z00q36388Sy5o9Df3EKPsvPdNqliPIIYGaZtIBdSlRSg7-fpVDengaNUDMCgWaaHt0vTN78Yzw0ta0T0_ZKyeIeF-g_xR0aYfMSrjsfnlB-slheTkNTMkiP3xMT1d8cY53ha8gI6vz4qTNljkU0KLMt8SDd9CU4KTItRPQJ0J9FgrvaWn8FLg0-dFinBjqPvtiLfuLMlNmDtgzMDSKErA/file [following]\n",
            "--2025-12-08 09:37:14--  https://uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com/cd/0/inline2/C2qLcOGWYzeNHxAZ__Uqu830ppDxiX5UEth-lwH6C3n6bZb-7R9aQTg9GmAc0SD8buBiWEjRsiFZwBvNsZBX9uaw7GUD2ZR9dJHJ5MpaPOGZxR8u_CQhEjsQVTTWd1OgVPZh0FBv25HnKjY96sa7ljYK-Z00q36388Sy5o9Df3EKPsvPdNqliPIIYGaZtIBdSlRSg7-fpVDengaNUDMCgWaaHt0vTN78Yzw0ta0T0_ZKyeIeF-g_xR0aYfMSrjsfnlB-slheTkNTMkiP3xMT1d8cY53ha8gI6vz4qTNljkU0KLMt8SDd9CU4KTItRPQJ0J9FgrvaWn8FLg0-dFinBjqPvtiLfuLMlNmDtgzMDSKErA/file\n",
            "Reusing existing connection to uc87e89067ba8486f1c245e80b35.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 166283563 (159M) [application/zip]\n",
            "Saving to: \u2018celeba64png_val.zip\u2019\n",
            "\n",
            "celeba64png_val.zip 100%[===================>] 158.58M  11.7MB/s    in 13s     \n",
            "\n",
            "2025-12-08 09:37:27 (12.1 MB/s) - \u2018celeba64png_val.zip\u2019 saved [166283563/166283563]\n",
            "\n",
            "19867\n"
          ]
        }
      ],
      "source": [
        "# do just once:\n",
        "!wget -nc -O celeba64png_val.zip 'https://www.dropbox.com/scl/fi/3d2le2wlu61nzkbxymfm3/celeba64png_val.zip?rlkey=ckesud01kwb8tualsd3s8zg5d'\n",
        "!unzip -nq celeba64png_val.zip\n",
        "!ls val | wc -l # there should be 19867 files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eba87051",
      "metadata": {
        "id": "eba87051",
        "outputId": "0b7bf3b0-b8c2-4eab-c9c0-c2780907abe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "from datetime import datetime\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4d66fc5c",
      "metadata": {
        "id": "4d66fc5c"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "class PngImageFolderDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_dir):\n",
        "        self.img_paths = sorted(glob(os.path.join(image_dir, \"*.png\")))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torchvision.transforms.ToTensor()(Image.open(self.img_paths[idx]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "train_ds = PngImageFolderDataset('val')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb808489",
      "metadata": {
        "id": "bb808489"
      },
      "source": [
        "# Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1982c0e",
      "metadata": {
        "id": "e1982c0e"
      },
      "source": [
        "## Losses for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "686820d6",
      "metadata": {
        "id": "686820d6",
        "outputId": "7376ade6-253a-4e41-8969-6084b002ce91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 14 (ipython-input-2056890792.py, line 18)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2056890792.py\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    def reparameterize(mu, std):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 14\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# utils functions for training and sampling:\n",
        "\n",
        "def kl(mu, log_var):\n",
        "    \"\"\"KL divergence to a standard normal prior for diagonal Gaussian q(z|x).\"\"\"\n",
        "    loss = -0.5 * torch.sum(1 + log_var - mu ** 2 - torch.exp(log_var), dim=[1, 2, 3])\n",
        "    return torch.mean(loss, dim=0)\n",
        "\n",
        "\n",
        "def kl_delta(delta_mu, delta_log_var, mu, log_var):\n",
        "    \"\"\"\n",
        "    KL(q || p) between two diagonal Gaussians where\n",
        "      p = N(mu, exp(log_var))\n",
        "      q = N(mu + delta_mu, exp(log_var + delta_log_var)).\n",
        "    Returns mean KL over the batch.\n",
        "    \"\"\"\n",
        "    mu_q = mu + delta_mu\n",
        "    log_var_q = log_var + delta_log_var\n",
        "\n",
        "    kl_term = 0.5 * (\n",
        "        log_var - log_var_q\n",
        "        + torch.exp(log_var_q - log_var)\n",
        "        + (mu_q - mu) ** 2 / torch.exp(log_var)\n",
        "        - 1.0\n",
        "    )\n",
        "    return kl_term.sum(dim=[1, 2, 3]).mean()\n",
        "\n",
        "\n",
        "def reparameterize(mu, std):\n",
        "    z = torch.randn_like(mu) * std + mu\n",
        "    return z\n",
        "\n",
        "\n",
        "from torch.nn.utils import spectral_norm\n",
        "def add_sn(m):\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
        "        return spectral_norm(m)\n",
        "    else:\n",
        "        return m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98354948",
      "metadata": {
        "id": "98354948"
      },
      "source": [
        "### Common layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18bbd515",
      "metadata": {
        "id": "18bbd515"
      },
      "outputs": [],
      "source": [
        "#from nvae.common import Swish, DecoderResidualBlock, ResidualBlock\n",
        "\n",
        "class Swish(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "\n",
        "class DecoderResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, n_group):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "            nn.Conv2d(dim, n_group * dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, n_group * dim, kernel_size=5, padding=2, groups=n_group),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self._seq(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self._seq = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim), Swish(),\n",
        "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self._seq(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d9c069",
      "metadata": {
        "id": "03d9c069"
      },
      "source": [
        "### Encoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82bf8a78",
      "metadata": {
        "id": "82bf8a78"
      },
      "outputs": [],
      "source": [
        "# Encoder:\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(out_channel, out_channel // 2, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_channel // 2), Swish(),\n",
        "            nn.Conv2d(out_channel // 2, out_channel, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channel), Swish()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._seq(x)\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        modules = []\n",
        "        for i in range(len(channels) - 1):\n",
        "            modules.append(ConvBlock(channels[i], channels[i + 1]))\n",
        "\n",
        "        self.modules_list = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.modules_list:\n",
        "            x = module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(dim, dim, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim), Swish(),\n",
        "            nn.Conv2d(dim, dim, kernel_size=3, padding=1),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self.seq(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock([3, z_dim // 16, z_dim // 8]),  # (16, 16)\n",
        "            EncoderBlock([z_dim // 8, z_dim // 4, z_dim // 2]),  # (4, 4)\n",
        "            EncoderBlock([z_dim // 2, z_dim]),  # (2, 2)\n",
        "        ])\n",
        "\n",
        "        self.encoder_residual_blocks = nn.ModuleList([\n",
        "            EncoderResidualBlock(z_dim // 8),\n",
        "            EncoderResidualBlock(z_dim // 2),\n",
        "            EncoderResidualBlock(z_dim),\n",
        "        ])\n",
        "\n",
        "        self.condition_x = nn.Sequential(\n",
        "            Swish(),\n",
        "            nn.Conv2d(z_dim, z_dim * 2, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        xs = []\n",
        "        for e, r in zip(self.encoder_blocks, self.encoder_residual_blocks):\n",
        "            x = r(e(x))\n",
        "            xs.append(x)\n",
        "\n",
        "        mu, log_var = self.condition_x(x).chunk(2, dim=1)\n",
        "\n",
        "        return mu, log_var, xs[:-1][::-1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb85966",
      "metadata": {
        "id": "edb85966"
      },
      "source": [
        "### Decoder architecture with shared top-down encoder/decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9875c6",
      "metadata": {
        "id": "ae9875c6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Decoder:\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(in_channel,\n",
        "                               out_channel,\n",
        "                               kernel_size=3,\n",
        "                               stride=2,\n",
        "                               padding=1,\n",
        "                               output_padding=1),\n",
        "            # nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            # nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channel), Swish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._seq(x)\n",
        "\n",
        "\n",
        "class DecoderResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, n_group):\n",
        "        super().__init__()\n",
        "\n",
        "        self._seq = nn.Sequential(\n",
        "            nn.Conv2d(dim, n_group * dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, n_group * dim, kernel_size=5, padding=2, groups=n_group),\n",
        "            nn.BatchNorm2d(n_group * dim), Swish(),\n",
        "            nn.Conv2d(n_group * dim, dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(dim),\n",
        "            SELayer(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + 0.1 * self._seq(x)\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        modules = []\n",
        "        for i in range(len(channels) - 1):\n",
        "            modules.append(UpsampleBlock(channels[i], channels[i + 1]))\n",
        "        self.module_list = nn.ModuleList(modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for module in self.module_list:\n",
        "            x = module(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Input channels = z_channels * 2 = x_channels + z_channels\n",
        "        # Output channels = z_channels\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock([z_dim * 2, z_dim // 2]),  # 2x upsample\n",
        "            DecoderBlock([z_dim, z_dim // 4, z_dim // 8]),  # 4x upsample\n",
        "            DecoderBlock([z_dim // 4, z_dim // 16, z_dim // 32])  # 4x uplsampe\n",
        "        ])\n",
        "        self.decoder_residual_blocks = nn.ModuleList([\n",
        "            DecoderResidualBlock(z_dim // 2, n_group=4),\n",
        "            DecoderResidualBlock(z_dim // 8, n_group=2),\n",
        "            DecoderResidualBlock(z_dim // 32, n_group=1)\n",
        "        ])\n",
        "\n",
        "        # p(z_l | z_(l-1))\n",
        "        self.condition_z = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim // 2),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 2, z_dim, kernel_size=1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim // 8),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 8, z_dim // 4, kernel_size=1)\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        # p(z_l | x, z_(l-1))\n",
        "        self.condition_xz = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim),\n",
        "                nn.Conv2d(z_dim, z_dim // 2, kernel_size=1),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 2, z_dim, kernel_size=1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                ResidualBlock(z_dim // 4),\n",
        "                nn.Conv2d(z_dim // 4, z_dim // 8, kernel_size=1),\n",
        "                Swish(),\n",
        "                nn.Conv2d(z_dim // 8, z_dim // 4, kernel_size=1)\n",
        "            )\n",
        "        ])\n",
        "\n",
        "        self.recon = nn.Sequential(\n",
        "            ResidualBlock(z_dim // 32),\n",
        "            nn.Conv2d(z_dim // 32, 3, kernel_size=1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, z, xs=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param z: shape. = (B, z_dim, map_h, map_w)\n",
        "        if xs=None: sample mode; otherwise xs is list of intermediate encoder features\n",
        "        \"\"\"\n",
        "\n",
        "        B, D, map_h, map_w = z.shape\n",
        "\n",
        "        # The init h (hidden state), can be replace with learned param, but it didn't work much\n",
        "        decoder_out = torch.zeros(B, D, map_h, map_w, device=z.device, dtype=z.dtype)\n",
        "\n",
        "        kl_losses = []\n",
        "\n",
        "        for i in range(len(self.decoder_residual_blocks)):\n",
        "\n",
        "            z_sample = torch.cat([decoder_out, z], dim=1)\n",
        "            decoder_out = self.decoder_residual_blocks[i](self.decoder_blocks[i](z_sample))\n",
        "\n",
        "            if i == len(self.decoder_residual_blocks) - 1: # stop if last block\n",
        "                break\n",
        "\n",
        "            mu, log_var = self.condition_z[i](decoder_out).chunk(2, dim=1) # parameter for sampling next z\n",
        "\n",
        "            if xs is not None:\n",
        "                delta_mu, delta_log_var = self.condition_xz[i](\n",
        "                                torch.cat([xs[i], decoder_out], dim=1)).chunk(2, dim=1)\n",
        "                kl_losses.append(kl_delta(delta_mu, delta_log_var, mu, log_var))\n",
        "                mu = mu + delta_mu\n",
        "                log_var = log_var + delta_log_var\n",
        "\n",
        "            z = reparameterize(mu, torch.exp(0.5 * log_var))\n",
        "\n",
        "        x_hat = torch.sigmoid(self.recon(decoder_out))\n",
        "\n",
        "        return x_hat, kl_losses\n",
        "\n",
        "\n",
        "    def sample(self, n_samples=32, fix_level=-1):\n",
        "        \"\"\"\n",
        "        Sample from the hierarchical prior.\n",
        "\n",
        "        Args:\n",
        "            n_samples: number of images to sample.\n",
        "            fix_level: -1 = all latents random;\n",
        "                       0 = share top latent across samples;\n",
        "                       1 = share top two latents across samples.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        device = next(self.parameters()).device\n",
        "        with torch.no_grad():\n",
        "            B = n_samples\n",
        "            # top latent (z0)\n",
        "            if fix_level >= 0:\n",
        "                z0_single = torch.randn(1, self.z_dim, 2, 2, device=device)\n",
        "                z = z0_single.expand(B, -1, -1, -1)\n",
        "            else:\n",
        "                z = torch.randn(B, self.z_dim, 2, 2, device=device)\n",
        "\n",
        "            decoder_out = torch.zeros_like(z)\n",
        "            fixed_latents = {}\n",
        "\n",
        "            for i in range(len(self.decoder_residual_blocks)):\n",
        "                z_sample = torch.cat([decoder_out, z], dim=1)\n",
        "                decoder_out = self.decoder_residual_blocks[i](self.decoder_blocks[i](z_sample))\n",
        "\n",
        "                if i == len(self.decoder_residual_blocks) - 1:\n",
        "                    break\n",
        "\n",
        "                mu, log_var = self.condition_z[i](decoder_out).chunk(2, dim=1)\n",
        "                std = torch.exp(0.5 * log_var)\n",
        "\n",
        "                level = i + 1  # next latent level being sampled\n",
        "                if fix_level >= level:\n",
        "                    if level not in fixed_latents:\n",
        "                        fixed_latents[level] = reparameterize(mu, std)[:1]\n",
        "                    z = fixed_latents[level].expand(B, -1, -1, -1)\n",
        "                else:\n",
        "                    z = reparameterize(mu, std)\n",
        "\n",
        "            x_hat = torch.sigmoid(self.recon(decoder_out))\n",
        "            return x_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e80077a",
      "metadata": {
        "id": "8e80077a"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NVAE(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim, img_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(z_dim)\n",
        "        self.decoder = Decoder(z_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "\n",
        "        :param x: Tensor. shape = (B, C, H, W)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        mu, log_var, xs = self.encoder(x)\n",
        "\n",
        "        # (B, D_Z)\n",
        "        z = reparameterize(mu, torch.exp(0.5 * log_var)) # sampling top latent variable\n",
        "\n",
        "        decoder_output, kl_losses = self.decoder(z, xs)\n",
        "\n",
        "        kl_losses = [kl(mu, log_var)]+kl_losses\n",
        "\n",
        "        recon_loss = nn.MSELoss(reduction='sum')(decoder_output, x)/decoder_output.shape[0]\n",
        "\n",
        "        return decoder_output, recon_loss, kl_losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization functions"
      ],
      "metadata": {
        "id": "duat51BAd7gP"
      },
      "id": "duat51BAd7gP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b736f686",
      "metadata": {
        "id": "b736f686"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "def imshow(img):\n",
        "    #img = img*0.5 + 0.5     # unnormalize\n",
        "    pil_img = torchvision.transforms.functional.to_pil_image(img)\n",
        "    display(pil_img)\n",
        "    #print(\"Image size (h x w): \",  pil_img.height, \"x\", pil_img.width)\n",
        "    return(pil_img)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "model = NVAE(z_dim=512, img_dim=(64, 64)).to(device)\n",
        "\n",
        "def show_decoder_output(z=None):\n",
        "  # provide random latent code as option to see evolution\n",
        "  with torch.no_grad():\n",
        "    if z==None:\n",
        "      z = torch.randn((batch_size,512,2,2)).to(device)\n",
        "      # We use full batch size and then select first 32 images\n",
        "    genimages = model.decoder(z)[0]\n",
        "    pil_img = imshow(torchvision.utils.make_grid(genimages[:32,:,:,:].to('cpu'),nrow=8))\n",
        "  return(pil_img)\n",
        "\n",
        "show_decoder_output();\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75fc56ea",
      "metadata": {
        "id": "75fc56ea"
      },
      "source": [
        "# Training\n",
        "\n",
        "### Exercise 2:\n",
        "1. Display an image of 4x8 portraits from the training dataset.\n",
        "1. Read the model architecture and train for 5 epochs.\n",
        "1. How do you explain the difference of images generated with and without model.eval()?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display an image grid (4x8) from the training dataset.\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "preview_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "images = next(iter(preview_loader))\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.axis('off')\n",
        "plt.imshow(make_grid(images[:32], nrow=8).permute(1, 2, 0))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8bE9SXI9E8r3"
      },
      "id": "8bE9SXI9E8r3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f16c5d",
      "metadata": {
        "scrolled": false,
        "id": "e8f16c5d"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 128\n",
        "n_cpu = 2\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=n_cpu)\n",
        "\n",
        "model = NVAE(z_dim=512, img_dim=(64, 64)).to(device)\n",
        "# apply Spectral Normalization\n",
        "model.apply(add_sn)\n",
        "\n",
        "\n",
        "zshow = torch.randn((batch_size,512,2,2)).to(device)\n",
        "\n",
        "\n",
        "# folder for checkpoints and visualization:\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "checkpoints_dir = dt_string+\"_checkpoints\"\n",
        "os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "outputs_dir = dt_string+\"_outputs\"\n",
        "os.makedirs(outputs_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15, eta_min=1e-4)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    for i, image in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        image = image.to(device)\n",
        "        image_recon, recon_loss, kl_losses = model(image)\n",
        "\n",
        "        kl_f = 1.\n",
        "        loss = recon_loss + kl_f*sum(kl_losses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            log_str = \"\\r---- [Epoch %d/%d, Step %d/%d] loss: %.6f----\" % (\n",
        "            epoch, epochs, i, len(train_dataloader), loss.item())\n",
        "            print(log_str)\n",
        "            with torch.no_grad():\n",
        "                pil_img = show_decoder_output(zshow)\n",
        "                imgpath = os.path.join(outputs_dir, \"nvae_simple_loss_epoch_\"+str(epoch).zfill(3)+\"_step_\"+str(i).zfill(4)+\".png\")\n",
        "                pil_img.save(imgpath)\n",
        "                model.eval()\n",
        "                pil_img = show_decoder_output(zshow)\n",
        "                imgpath = os.path.join(outputs_dir, \"nvae_simple_loss_epoch_\"+str(epoch).zfill(3)+\"_step_\"+str(i).zfill(4)+\"_eval.png\")\n",
        "                pil_img.save(imgpath)\n",
        "                model.train()\n",
        "\n",
        "    # end epoch: save checkpoint:\n",
        "    scheduler.step()\n",
        "    if epoch%5==4:\n",
        "      checkpoint_path = os.path.join(checkpoints_dir, \"nvae_simple_loss_epoch_\"+str(epoch).zfill(3)+\".pth\")\n",
        "      torch.save(model.state_dict(), checkpoint_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d022f1",
      "metadata": {
        "id": "d4d022f1"
      },
      "source": [
        "### Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7df4ea",
      "metadata": {
        "id": "7c7df4ea"
      },
      "outputs": [],
      "source": [
        "# Load pretrained model:\n",
        "!wget -nc -O nvae_simple_loss_epoch_199.pth 'https://www.dropbox.com/scl/fi/0rsjcx78w338nj4ie6lun/nvae_simple_loss_epoch_199.pth?rlkey=8ok3htl9gp3ywmpqu02xdfo87'\n",
        "checkpoint_path = 'nvae_simple_loss_epoch_199.pth'\n",
        "model = NVAE(z_dim=512, img_dim=(64, 64)).to(device)\n",
        "model.apply(add_sn)\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
        "model.eval()\n",
        "\n",
        "zshow = torch.randn((batch_size,512,2,2)).to(device)\n",
        "show_decoder_output(zshow);\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae8a7d2",
      "metadata": {
        "id": "9ae8a7d2"
      },
      "source": [
        "### Exercise 3:\n",
        "1. Complete the function def sample(self, n_samples=32, fix_level=-1) of the encoder class so that it samples images with common realizations up to fix_level (begin by implementing and testing independent sampling).\n",
        "1. Test this function and verify the hierarchical VAE encodes the images hierarchically.\n",
        "1. (Bonus question) Sample 20k images and compute FID against the celeba test set (available here https://www.dropbox.com/scl/fi/in8hqobto2p2k2baiwi5x/celeba64png_test.zip?rlkey=jmisq9swucjwjyv69ftwwks06)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1860072e",
      "metadata": {
        "scrolled": true,
        "id": "1860072e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sampling with the (pre)trained hierarchical VAE\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # change fix_level to 0 or 1 to share higher-level latents across samples\n",
        "    samples = model.decoder.sample(n_samples=32, fix_level=-1)\n",
        "    grid = torchvision.utils.make_grid(samples.cpu(), nrow=8)\n",
        "    display(torchvision.transforms.functional.to_pil_image(grid))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch21",
      "language": "python",
      "name": "pytorch21"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}