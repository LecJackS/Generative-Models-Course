{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1429a14e",
   "metadata": {},
   "source": [
    "# Exercise A - VAEs\n",
    "\n",
    "In this exercise, you will implement the key mathematical components of a VAE in a controlled setting.\n",
    "\n",
    "You must implement **three** functions:\n",
    "\n",
    "1. `reparameterize(mu, logvar)`\n",
    "2. `kl_diag_gaussian_to_standard(mu, logvar)`\n",
    "3. `bernoulli_nll_from_logits(logits, x)`\n",
    "\n",
    "Then you will combine them in a provided `vae_loss` function and run sanity checks.\n",
    "\n",
    "Rules:\n",
    "- Do not change the test code.\n",
    "- Your functions must work for generic batch size $B$ and latent dimension $D$.\n",
    "- Use $p(z) = \\mathcal{N}(0, I)$ and a diagonal Gaussian $q(z \\mid x)$.\n",
    "\n",
    "When you are done, answer the short conceptual questions at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Setup (do not modify this cell)\n",
    "# --------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Optional: reduce flakiness across some GPU setups (should not matter for this exercise)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def assert_close(a: torch.Tensor, b: torch.Tensor, tol: float, msg: str):\n",
    "    err = (a - b).abs().max().item()\n",
    "    assert err <= tol, f\"{msg} | max abs error = {err:.4e} > tol = {tol:.4e}\"\n",
    "\n",
    "# Small synthetic \"batch\"\n",
    "B, D, P = 64, 5, 32  # batch, latent dim, \"pixels/features\"\n",
    "x = torch.bernoulli(torch.full((B, P), 0.3, device=device))  # x in {0,1}\n",
    "logits = torch.randn(B, P, device=device) * 2.0             # arbitrary logits\n",
    "\n",
    "mu = torch.randn(B, D, device=device)\n",
    "logvar = torch.randn(B, D, device=device) * 0.3  # keep variances not too extreme\n",
    "\n",
    "print(\n",
    "    \"Shapes:\",\n",
    "    \"x\", x.shape,\n",
    "    \"| logits\", logits.shape,\n",
    "    \"| mu\", mu.shape,\n",
    "    \"| logvar\", logvar.shape,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reparameterization trick.\n",
    "\n",
    "    Given mu and log-variance, sample z\n",
    "\n",
    "    Args:\n",
    "        mu:     shape [B, D]\n",
    "        logvar: shape [B, D]\n",
    "\n",
    "    Returns:\n",
    "        z: shape [B, D]\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1) Convert log-variance to standard deviation\n",
    "    # 2) Sample eps ~ N(0, I) with same shape as std (or mu)\n",
    "    std = torch.exp(None) # TODO: Implement\n",
    "    eps = None # TODO: Implement\n",
    "    z = None # TODO: Implement\n",
    "    raise NotImplementedError # remove this line and return the correct value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Tests (do not modify this cell)\n",
    "# --------------------------------\n",
    "torch.manual_seed(123)\n",
    "B_test, D_test = 50000, 2\n",
    "mu_test = torch.tensor([[1.5, -0.5]], device=device).expand(B_test, D_test)\n",
    "sigma2_test = torch.tensor([[0.5, 1.5]], device=device).expand(B_test, D_test)\n",
    "logvar_test = torch.log(sigma2_test)\n",
    "\n",
    "z = reparameterize(mu_test, logvar_test)\n",
    "\n",
    "emp_mean = z.mean(dim=0)\n",
    "emp_var = z.var(dim=0, unbiased=False)\n",
    "\n",
    "target_mean = mu_test[0]\n",
    "target_var = sigma2_test[0]\n",
    "\n",
    "assert_close(emp_mean, target_mean, tol=3e-2, msg=\"reparameterize: mean mismatch\")\n",
    "assert_close(emp_var, target_var, tol=5e-2, msg=\"reparameterize: variance mismatch\")\n",
    "\n",
    "# Gradient sanity check\n",
    "mu_small = torch.randn(4, 3, device=device, requires_grad=True)\n",
    "logvar_small = (torch.randn(4, 3, device=device, requires_grad=True) * 0.1).detach().requires_grad_()\n",
    "z_small = reparameterize(mu_small, logvar_small)\n",
    "loss = (z_small**2).mean()\n",
    "loss.backward()\n",
    "\n",
    "assert torch.isfinite(mu_small.grad).all(), \"reparameterize: non-finite grad wrt mu\"\n",
    "assert torch.isfinite(logvar_small.grad).all(), \"reparameterize: non-finite grad wrt logvar\"\n",
    "\n",
    "print(\"OK - reparameterize passed all tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf44ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_diag_gaussian_to_standard(mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    KL(q || p) where:\n",
    "      q = N(mu, diag(sigma^2)),  p = N(0, I)\n",
    "\n",
    "    Closed form per example\n",
    "\n",
    "    Args:\n",
    "        mu:     shape [B, D]\n",
    "        logvar: shape [B, D]\n",
    "\n",
    "    Returns:\n",
    "        kl_per_example: shape [B]  (sum over dims, no batch reduction)\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1) Compute sigma^2\n",
    "    # 2) Compute per-dim KL\n",
    "    # 3) Sum over latent dims to get per-example KL\n",
    "    sigma2 = None        # TODO: Implement\n",
    "    kl_per_dim = None    # TODO: Implement\n",
    "    kl_per_example = kl_per_dim.sum(dim=1)\n",
    "    raise NotImplementedError # remove this line and return the correct value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Tests (do not modify)\n",
    "# -------------------------\n",
    "\n",
    "# Known-value test (hand-checked)\n",
    "mu_k = torch.tensor([[2.0, 0.0]], device=device)\n",
    "logvar_k = torch.log(torch.tensor([[0.5, 1.0]], device=device))  # sigma^2 = [0.5, 1.0]\n",
    "\n",
    "kl_k = kl_diag_gaussian_to_standard(mu_k, logvar_k)  # shape [1]\n",
    "expected = torch.tensor([2.0965735], device=device)  # precomputed\n",
    "\n",
    "assert_close(kl_k, expected, tol=1e-5, msg=\"KL known-value test failed\")\n",
    "\n",
    "# Zero KL test\n",
    "mu0 = torch.zeros(10, 3, device=device)\n",
    "logvar0 = torch.zeros(10, 3, device=device)\n",
    "kl0 = kl_diag_gaussian_to_standard(mu0, logvar0)\n",
    "assert_close(kl0, torch.zeros_like(kl0), tol=1e-6, msg=\"KL should be ~0 for standard normal\")\n",
    "\n",
    "# Non-negativity (numerical tolerance)\n",
    "mu_r = torch.randn(128, 7, device=device)\n",
    "logvar_r = torch.randn(128, 7, device=device) * 0.2\n",
    "kl_r = kl_diag_gaussian_to_standard(mu_r, logvar_r)\n",
    "assert kl_r.min().item() >= -1e-6, \"KL should be non-negative (up to small numerical error)\"\n",
    "\n",
    "print(\"OK - KL function passed all tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_nll_from_logits(logits: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for Bernoulli likelihood with logits.\n",
    "\n",
    "    For each example:\n",
    "      NLL(x; logits) = - sum_j log Bernoulli(x_j; sigmoid(logits_j))\n",
    "\n",
    "    Implement using BCE-with-logits (stable).\n",
    "    Return per-example NLL (sum over features, no batch reduction).\n",
    "\n",
    "    Args:\n",
    "        logits: shape [B, P]\n",
    "        x:      shape [B, P] with values in {0,1} or [0,1]\n",
    "\n",
    "    Returns:\n",
    "        nll_per_example: shape [B]\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1) Use F.binary_cross_entropy_with_logits\n",
    "    # 2) This returns per-feature loss [B, P]\n",
    "    # 3) Sum over features to get per-example NLL [B]\n",
    "    bce_per_feature = None      # TODO: Implement\n",
    "    nll_per_example = bce_per_feature.sum(dim=1)\n",
    "    raise NotImplementedError # remove this line and return the correct value\n",
    "\n",
    "\n",
    "def vae_loss(\n",
    "    logits: torch.Tensor,\n",
    "    x: torch.Tensor,\n",
    "    mu: torch.Tensor,\n",
    "    logvar: torch.Tensor,\n",
    "    beta: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      loss (scalar), recon (scalar), kl (scalar), kl_per_dim (shape [D])\n",
    "    \"\"\"\n",
    "    nll_per_ex = bernoulli_nll_from_logits(logits, x)         # [B]\n",
    "    kl_per_ex = kl_diag_gaussian_to_standard(mu, logvar)      # [B]\n",
    "\n",
    "    recon = nll_per_ex.mean()\n",
    "    kl = kl_per_ex.mean()\n",
    "    loss = recon + beta * kl\n",
    "\n",
    "    # Diagnostic: mean KL per dimension across batch\n",
    "    # KL per dim: 0.5 * (exp(logvar) + mu^2 - 1 - logvar), then average over batch\n",
    "    kl_per_dim = 0.5 * (logvar.exp() + mu.pow(2) - 1.0 - logvar)  # [B, D]\n",
    "    kl_per_dim = kl_per_dim.mean(dim=0)                            # [D]\n",
    "\n",
    "    return loss, recon, kl, kl_per_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007bcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Tests (do not modify)\n",
    "# -------------------------\n",
    "\n",
    "# NLL matches the stable PyTorch reference if implemented with BCE-with-logits\n",
    "ref = F.binary_cross_entropy_with_logits(logits, x, reduction=\"none\").sum(dim=1)\n",
    "out = bernoulli_nll_from_logits(logits, x)\n",
    "assert_close(out, ref, tol=1e-7, msg=\"Bernoulli NLL mismatch\")\n",
    "\n",
    "# Beta behavior sanity check\n",
    "loss0, recon0, kl0, _ = vae_loss(logits, x, mu, logvar, beta=0.0)\n",
    "loss1, recon1, kl1, _ = vae_loss(logits, x, mu, logvar, beta=1.0)\n",
    "\n",
    "assert_close(loss0, recon0, tol=1e-7, msg=\"beta=0 should give loss=recon\")\n",
    "assert (loss1 + 1e-10 >= loss0).item(), \"Increasing beta should not decrease total loss (same batch).\"\n",
    "\n",
    "print(\"OK - Bernoulli NLL and VAE loss checks passed.\")\n",
    "\n",
    "# Small diagnostic print (not graded, but useful)\n",
    "loss, recon, kl, kl_per_dim = vae_loss(logits, x, mu, logvar, beta=1.0)\n",
    "active_dims = int((kl_per_dim > 0.05).sum().item())\n",
    "\n",
    "print(f\"Example batch metrics: recon={recon.item():.3f}, kl={kl.item():.3f}, active_dims={active_dims}/{kl_per_dim.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fad5f",
   "metadata": {},
   "source": [
    "## Short questions (write short answers)\n",
    "\n",
    "**Question 1.** Your code computes `kl_per_dim`. If one latent dimension has much larger KL than the others on a batch, what does that suggest about how the encoder is using the latent space?\n",
    "\n",
    "**Answer 1.** \n",
    "\n",
    "---\n",
    "\n",
    "**Question 2.** In this exercise, the reconstruction term is a **sum** over features (per example). If we instead averaged over features, how would that affect the relative importance of the KL term when the number of features $P$ changes?\n",
    "\n",
    "**Answer 2.** \n",
    "\n",
    "---\n",
    "\n",
    "**Question 3.** Consider `logvar = -10` for some dimension. What does that imply about the posterior variance in that dimension, and why can this create numerical or optimization issues?\n",
    "\n",
    "**Answer 3.**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4.** In one sentence: what pressure does increasing $\\beta$ (the KL weight) put on `mu` and `logvar` during training?\n",
    "\n",
    "**Answer 4.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d40f4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bc8f9",
   "metadata": {},
   "source": [
    "# Exercise B - Continuous diffusion\n",
    "\n",
    "In this exercise, you will implement the forward noising step used in score-based diffusion models (VE-style), and the corresponding **conditional score target** used in denoising score matching.\n",
    "\n",
    "We use the perturbation model:\n",
    "$ x_t = x_0 + \\sigma(t)\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I). $\n",
    "\n",
    "You must implement **two** functions:\n",
    "\n",
    "1. `sigma_ve(t, sigma_min, sigma_max)`\n",
    "2. `ve_forward(x0, t, sigma_min, sigma_max)`\n",
    "\n",
    "and **one** score function:\n",
    "\n",
    "3. `conditional_score_xt_given_x0(xt, x0, sigma_t)`\n",
    "\n",
    "Then you will run sanity checks (do not modify them).\n",
    "\n",
    "Your code should work for general batch size $B$ and dimension $d$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f194d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Setup (do not modify this cell)\n",
    "# --------------------------------\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "def assert_close(a: torch.Tensor, b: torch.Tensor, tol: float, msg: str):\n",
    "    err = (a - b).abs().max().item()\n",
    "    assert err <= tol, f\"{msg} | max abs error = {err:.4e} > tol = {tol:.4e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6df6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_ve(\n",
    "    t: torch.Tensor,\n",
    "    sigma_min: float = 0.01,\n",
    "    sigma_max: float = 50.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    VE (variance exploding) noise schedule sigma(t)\n",
    "\n",
    "    Args:\n",
    "        t: tensor or float, shape [] or [B] or [B, 1]\n",
    "    Returns:\n",
    "        sigma(t) with the same shape as t\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1) Convert t to a float tensor\n",
    "    # 2) Compute quotient\n",
    "    # 3) Return sigma(t)\n",
    "    t = torch.as_tensor(t, dtype=torch.float32, device=device)\n",
    "    c = None # TODO: Implement\n",
    "    raise NotImplementedError # remove this line and return the correct value\n",
    "\n",
    "\n",
    "def ve_forward(\n",
    "    x0: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    sigma_min: float = 0.01,\n",
    "    sigma_max: float = 50.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward perturbation kernel for VE-style diffusion:\n",
    "      x_t = x_0 + sigma(t) * eps, eps ~ N(0, I).\n",
    "\n",
    "    Args:\n",
    "        x0: shape [B, d]\n",
    "        t:  shape [] or [B] (values in [0,1])\n",
    "\n",
    "    Returns:\n",
    "        xt:      shape [B, d]\n",
    "        eps:     shape [B, d]\n",
    "        sigma_t: shape [B, 1] (broadcastable to x0)\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1) sigma_t = sigma_ve(t)\n",
    "\n",
    "    x0 = x0.to(device)\n",
    "    B = x0.shape[0]\n",
    "\n",
    "    t = torch.as_tensor(t, dtype=torch.float32, device=device)\n",
    "    if t.ndim == 0:\n",
    "        t = t.expand(B)               # [B]\n",
    "    if t.ndim == 1:\n",
    "        t = t.view(B, 1)              # [B, 1]\n",
    "\n",
    "    sigma_t = None               # TODO: Implement # [B, 1]\n",
    "    eps = torch.randn_like(None) # TODO: Implement # [B, d]\n",
    "    xt = None                    # TODO: Implement # [B, d]\n",
    "    \n",
    "    return xt, eps, sigma_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Tests (do not modify)\n",
    "# -------------------------\n",
    "\n",
    "sigma_min, sigma_max = 0.01, 50.0\n",
    "\n",
    "# Endpoint checks for schedule\n",
    "s0 = sigma_ve(0.0, sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "s1 = sigma_ve(1.0, sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "assert_close(s0, torch.tensor(sigma_min, device=device), tol=1e-8, msg=\"sigma_ve(0) should be sigma_min\")\n",
    "assert_close(s1, torch.tensor(sigma_max, device=device), tol=1e-6, msg=\"sigma_ve(1) should be sigma_max\")\n",
    "\n",
    "# Shape/broadcast checks\n",
    "B, d = 8, 3\n",
    "x0 = torch.zeros(B, d, device=device)\n",
    "\n",
    "xt, eps, sigma_t = ve_forward(x0, t=0.25, sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "assert xt.shape == (B, d)\n",
    "assert eps.shape == (B, d)\n",
    "assert sigma_t.shape == (B, 1)\n",
    "\n",
    "xt2, eps2, sigma_t2 = ve_forward(\n",
    "    x0,\n",
    "    t=torch.full((B,), 0.25, device=device),\n",
    "    sigma_min=sigma_min,\n",
    "    sigma_max=sigma_max,\n",
    ")\n",
    "assert xt2.shape == (B, d)\n",
    "assert sigma_t2.shape == (B, 1)\n",
    "\n",
    "print(\"OK - sigma_ve and ve_forward passed basic tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_score_xt_given_x0(\n",
    "    xt: torch.Tensor,\n",
    "    x0: torch.Tensor,\n",
    "    sigma_t: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Conditional score of the perturbation kernel q(x_t | x_0) where\n",
    "      x_t = x_0 + sigma(t) * eps, eps ~ N(0, I).\n",
    "\n",
    "    Since q(x_t | x_0) = N(x_0, sigma(t)^2 I), we have:\n",
    "      ∇_{x_t} log q(x_t | x_0) = - (x_t - x_0) / sigma(t)^2.\n",
    "\n",
    "    Args:\n",
    "        xt:      shape [B, d]\n",
    "        x0:      shape [B, d]\n",
    "        sigma_t: shape [B, 1] (broadcastable)\n",
    "\n",
    "    Returns:\n",
    "        score: shape [B, d]\n",
    "    \"\"\"\n",
    "    # Hint:\n",
    "    # 1) sigma2 = sigma_t squared\n",
    "    sigma2 = None # TODO: Implement\n",
    "    score = None  # TODO: Implement\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Tests (do not modify)\n",
    "# -------------------------\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "sigma_min, sigma_max = 0.01, 50.0\n",
    "\n",
    "# Variance check: if x0=0, then xt has Var = sigma(t)^2 in each dimension.\n",
    "B = 40000\n",
    "d = 2\n",
    "x0 = torch.zeros(B, d, device=device)\n",
    "t_scalar = 0.30\n",
    "xt, eps, sigma_t = ve_forward(x0, t_scalar, sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "\n",
    "emp_var = (xt - x0).var(dim=0, unbiased=False)             # [d]\n",
    "target_var = (sigma_ve(t_scalar, sigma_min=sigma_min, sigma_max=sigma_max) ** 2).expand_as(emp_var)  # [d]\n",
    "assert_close(emp_var, target_var, tol=5e-3, msg=\"Forward variance does not match sigma(t)^2\")\n",
    "\n",
    "# Autograd check: compare to gradient of log q(x_t | x_0)\n",
    "B = 2048\n",
    "d = 2\n",
    "x0 = torch.randn(B, d, device=device)\n",
    "t = torch.rand(B, device=device)  # [B]\n",
    "xt, eps, sigma_t = ve_forward(x0, t, sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "\n",
    "xt = xt.detach().requires_grad_(True)\n",
    "sigma2 = (sigma_t ** 2)  # [B, 1]\n",
    "\n",
    "quad = -0.5 * ((xt - x0) ** 2 / sigma2).sum(dim=1)  # [B]\n",
    "log_norm = -0.5 * d * torch.log(2 * torch.pi * sigma2.squeeze(1))  # [B]\n",
    "log_q = quad + log_norm\n",
    "\n",
    "grad = torch.autograd.grad(log_q.sum(), xt)[0]\n",
    "target = conditional_score_xt_given_x0(xt, x0, sigma_t)\n",
    "assert_close(grad, target, tol=2e-4, msg=\"Conditional score does not match autograd gradient\")\n",
    "\n",
    "print(\"OK - conditional_score_xt_given_x0 passed all tests.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431451c8",
   "metadata": {},
   "source": [
    "## Short questions\n",
    "\n",
    "**Question 1.** In this exercise you computed $\\nabla_{x_t}\\log q(x_t \\mid x_0)$. In one sentence, explain how this differs from the **marginal score** $\\nabla_{x_t}\\log p_t(x_t)$ that appears in diffusion models.\n",
    "\n",
    "**Answer 1.** \n",
    "\n",
    "---\n",
    "\n",
    "**Question 2.** In the VE forward process, what is the qualitative effect of increasing $\\sigma(t)$ on the relationship between $x_t$ and $x_0$?\n",
    "\n",
    "**Answer 2.** \n",
    "\n",
    "---\n",
    "\n",
    "**Question 3.** Why do you think time $t$ is sampled at random when training a diffusion model (instead of always using a fixed $t$)?\n",
    "\n",
    "**Answer 3.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ab8a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb43dd",
   "metadata": {},
   "source": [
    "# Exercise C - Discrete diffusion\n",
    "\n",
    "We now build a toy discrete analogue of diffusion. You will answer conceptual questions about it.\n",
    "\n",
    "- Continuous diffusion: $x_t = x_0 + \\sigma(t)\\epsilon$ and we train a network to denoise using a time-conditioned loss.\n",
    "- Here: each pixel is **discrete** in $\\{0,1\\}$, and we corrupt pixels by **random replacement**.\n",
    "\n",
    "Forward corruption at step $t$ (noise level $\\beta_t$), independently per pixel:\n",
    "- with probability $1-\\beta_t$: keep the pixel\n",
    "- with probability $\\beta_t$: replace it with a uniform random value in $\\{0,1\\}$ (that is, sample $\\mathrm{Bernoulli}(0.5)$)\n",
    "\n",
    "Note: when $K=2$, a replacement does **not** always flip the bit. With probability $1/2$ the new sampled value equals the old one.\n",
    "\n",
    "We train a small network to predict the original clean image $x_0$ from $(x_t, t)$ using **cross-entropy**.\n",
    "This mirrors the continuous exercise: corruption + time conditioning + a denoising loss (MSE there, cross-entropy here).\n",
    "\n",
    "Notes:\n",
    "- This is intentionally simple. It is not a full discrete diffusion model.\n",
    "- The goal is to reuse the same workflow (noise schedule + denoise objective) on discrete data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509d49a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note:** For this exercise, you do not need to modify the code. You will only use it to answer the questions that follow.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88077b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "K = 2\n",
    "T = 10\n",
    "BATCH = 128\n",
    "\n",
    "def make_noise_schedule(T: int, beta_min: float = 0.05, beta_max: float = 0.6) -> torch.Tensor:\n",
    "    # We use timesteps t in {0, ..., T-1} and set beta_t = betas[t].\n",
    "    return torch.linspace(beta_min, beta_max, T, device=device)\n",
    "\n",
    "betas = make_noise_schedule(T)\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "def binarize_mnist(x_float: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "    # x_float: [B, 1, 28, 28] in [0,1] -> x_disc: [B, 28, 28] in {0,1} (int64)\n",
    "    return (x_float.squeeze(1) > threshold).long()\n",
    "\n",
    "x_float, _ = next(iter(train_loader))\n",
    "x_disc = binarize_mnist(x_float.to(device))\n",
    "print(\"binarized batch:\", x_disc.shape, x_disc.dtype, \"unique:\", torch.unique(x_disc).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed091014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_discrete(x0: torch.Tensor, t: torch.Tensor, betas: torch.Tensor, K: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Discrete forward corruption: random replacement at step t.\n",
    "\n",
    "    Args:\n",
    "      x0:    [B,H,W] long in {0,...,K-1}\n",
    "      t:     [] or [B] long in {1,...,T}  (we do not use t=0 here)\n",
    "      betas: [T] noise schedule, we use beta_t = betas[t-1]\n",
    "      K:     number of classes\n",
    "\n",
    "    Returns:\n",
    "      x_t: [B,H,W] long\n",
    "    \"\"\"\n",
    "    # Make sure t is a long tensor on the same device as x0\n",
    "    t = torch.as_tensor(t, device=x0.device).long()\n",
    "\n",
    "    # If t is a scalar, expand it to [B]\n",
    "    B = x0.shape[0]\n",
    "    if t.ndim == 0:\n",
    "        t = t.expand(B)  # [B]\n",
    "\n",
    "    # Convert t into per-example beta_t of shape [B,1,1]\n",
    "    beta_t = betas.to(x0.device)[t - 1].view(B, 1, 1)  # [B,1,1]\n",
    "\n",
    "    # Sample a boolean mask \"replace\" with Bernoulli(beta_t) for each pixel\n",
    "    replace = torch.rand(x0.shape, device=x0.device) < beta_t  # [B,H,W] bool\n",
    "\n",
    "    # Sample replacement values u ~ Uniform{0,...,K-1} with shape [B,H,W]\n",
    "    u = torch.randint(low=0, high=K, size=x0.shape, device=x0.device)  # [B,H,W] long\n",
    "\n",
    "    # Corrupt x0 using values from u where replace=True\n",
    "    x_t = torch.where(replace, u, x0)\n",
    "    return x_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDiscreteDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected denoiser:\n",
    "      - flatten x_t, append t/T as one more feature\n",
    "      - MLP -> logits for every pixel/class, reshaped to [B,K,H,W]\n",
    "    \"\"\"\n",
    "    def __init__(self, K: int, T: int, hidden: int = 64, H: int = 28, W: int = 28):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        in_dim = H * W + 1\n",
    "        out_dim = K * H * W\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W = x_t.shape\n",
    "        assert H == self.H and W == self.W, \"input size mismatch\"\n",
    "        x_flat = x_t.float().reshape(B, -1)            # [B, H*W]\n",
    "        t_feat = (t.float() / self.T).unsqueeze(1)     # [B, 1]\n",
    "        inp = torch.cat([x_flat, t_feat], dim=1)       # [B, H*W+1]\n",
    "        logits = self.net(inp)                         # [B, K*H*W]\n",
    "        return logits.view(B, self.K, H, W)\n",
    "\n",
    "\n",
    "def train_overfit_one_batch(num_steps: int = 300, t_train: int = T, lr: float = 1e-3):\n",
    "    \"\"\"\n",
    "    Overfit a single batch at a fixed noise level.\n",
    "    We keep (x0, x_t) fixed to make the wiring check strict and fast.\n",
    "    \"\"\"\n",
    "    assert 1 <= t_train <= T, \"t_train must be in {1,...,T} for this exercise\"\n",
    "\n",
    "    # Reinitialize model/optimizer each time for repeatability\n",
    "    model = TinyDiscreteDenoiser(K=K, T=T).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    x_float, _ = next(iter(train_loader))\n",
    "    x0 = binarize_mnist(x_float.to(device))  # [B,28,28]\n",
    "\n",
    "    t = torch.full((x0.shape[0],), t_train, device=device, dtype=torch.long)\n",
    "    x_t = corrupt_discrete(x0, t, betas, K)\n",
    "\n",
    "    losses = []\n",
    "    for _ in tqdm(range(num_steps)):\n",
    "        opt.zero_grad()\n",
    "        logits = model(x_t, t)                  # [B,K,H,W]\n",
    "        loss = F.cross_entropy(logits, x0)      # scalar\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return model, x0, x_t, losses\n",
    "\n",
    "\n",
    "model, x0_fixed, xT_fixed, losses = train_overfit_one_batch(num_steps=300, t_train=T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_denoising_examples(model: nn.Module, x0: torch.Tensor, x_t: torch.Tensor, t: int, n: int = 8):\n",
    "    model.eval()\n",
    "    B = x0.shape[0]\n",
    "    t_vec = torch.full((B,), t, device=x0.device, dtype=torch.long)\n",
    "\n",
    "    logits = model(x_t, t_vec)              # [B,K,H,W]\n",
    "    xhat = logits.argmax(dim=1)             # [B,H,W]\n",
    "\n",
    "    acc_in = (x_t == x0).float().mean().item()\n",
    "    acc_out = (xhat == x0).float().mean().item()\n",
    "    print(f\"Pixel accuracy at t={t}: input x_t={acc_in:.3f} | predicted xhat={acc_out:.3f}\")\n",
    "\n",
    "    n = min(n, B)\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(7, 2 * n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, 3)\n",
    "\n",
    "    x0_cpu = x0[:n].cpu()\n",
    "    xt_cpu = x_t[:n].cpu()\n",
    "    xhat_cpu = xhat[:n].cpu()\n",
    "\n",
    "    for i in range(n):\n",
    "        axes[i, 0].imshow(x0_cpu[i], vmin=0, vmax=K - 1, cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"clean $x_0$\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(xt_cpu[i], vmin=0, vmax=K - 1, cmap=\"gray\")\n",
    "        axes[i, 1].set_title(f\"corrupted $x_t$ (t={t})\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(xhat_cpu[i], vmin=0, vmax=K - 1, cmap=\"gray\")\n",
    "        axes[i, 2].set_title(r\"predicted $\\hat{x}_0$\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# If you used the version that returns model:\n",
    "show_denoising_examples(model, x0_fixed, xT_fixed, t=T, n=5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training curve (overfitting one batch)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"cross-entropy\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c6754",
   "metadata": {},
   "source": [
    "## Short questions (write short answers)\n",
    "\n",
    "**Question 1.** In this discrete corruption process (binary MNIST, $K=2$), what is the analogue of “noise increasing” from the continuous case?\n",
    "\n",
    "**Answer 1.**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2.** What does $\\beta_t$ control, and what are the limiting behaviors at $\\beta_t=0$ and $\\beta_t=1$?\n",
    "\n",
    "**Answer 2.**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 3.** In the model input we concatenate the corrupted image $x_t$ and a “time channel” containing $t/T$. Why include $t$ explicitly?\n",
    "\n",
    "**Answer 3.**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 4.** This notebook overfits one fixed batch at a fixed $t=T$. What does a decreasing training loss show, and what does it not show?\n",
    "\n",
    "**Answer 4.**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 5.** After confirming the fixed-$t$ sanity check works, what is the immediate next step to make training closer to the continuous diffusion training you saw?\n",
    "\n",
    "**Answer 5.**\n",
    "\n",
    "---\n",
    "\n",
    "**Question 6.** In the VAE notebook, the objective had reconstruction + KL. In this discrete denoising toy example we only minimize cross-entropy. Which VAE term does cross-entropy correspond to, and what VAE term is missing? Also, compared to the continuous diffusion pipeline, what major ingredient is still missing if we want to *generate* samples from noise?\n",
    "\n",
    "**Answer 6.**\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
